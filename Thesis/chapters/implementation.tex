\section{Compiler}
\label{sec:compiler}
\setauthor{Luna Klatzer}

The Kipper Compiler is the core component of the Kipper project, serving as the central piece connecting the Kipper language to its target environment. It functions similarly to other \gls{transpilation}-based compilers, such as the TypeScript compiler, by producing high-level output code from high-level input codeâ€”specifically, code written in the Kipper language. The syntax of the language is predefined and implemented using the lexer and parser generated by the \Gls{antlr4} parser generator. 

An interesting aspect of the Kipper compiler is its largely modular design, which allows various components to be replaced or extended as needed. This modularity primarily serves to enable the compiler's structure to adapt to future changes in the language and its target output environment. Given the rapid evolution of web technologies and the frequent addition of new functionality, it is essential to ensure that the compiler remains current. Additionally, this design allows users to create plugins or extensions for the compiler to support custom functionality that may not be natively available.

Furthermore, as discussed in greater detail in section~\ref{sec:output-generation}, the compiler is capable of targeting more than one output format. Specifically, it supports both standard JavaScript, adhering to the ES6/ES2015 specification, and standard TypeScript as defined by Microsoft. Similar to other compiler systems, users can configure the compiler according to their preferences and specify the desired target language.

\subsection{Stages of compilation}
\setauthor{Luna Klatzer}

The Kipper compiler processes the program through multiple phases, each building upon the previous one to progressively enrich the semantic and logical representation of the program. The phases and their corresponding responsible components are as follows:

\subsubsection{Lexical Analysis}
\begin{itemize}
	\item Detailed explanation in section~\ref{sec:lexing-parsing}
	\item Performed by: Antlr4 Kipper Lexer
	\item This phase tokenizes the source code into a stream of lexemes, identifying the basic units of syntax.
\end{itemize}

\subsubsection{Syntax Analysis - Parsing}
\begin{itemize}
	\item Detailed explanation in section~\ref{sec:lexing-parsing}
	\item Performed by: Antlr4 Kipper Parser
	\item In this phase, the lexed tokens are organized into a parse tree based on the grammar rules of the Kipper language.
\end{itemize}

\subsubsection{Parse Tree Translation to AST}
\begin{itemize}
	\item Detailed explanation in section~\ref{sec:translation-to-the-ast}
	\item Performed by: Kipper Core Compiler
	\item Converts the parse tree into an Abstract Syntax Tree (AST), a more abstract and language-independent representation of the code.
\end{itemize}

\subsubsection{Semantic Analysis}
\begin{itemize}
	\item Detailed explanation in section~\ref{sec:semantic-analysis}
	\item Performed by: Kipper Core Compiler
	\item This phase is split into three separate steps:
	\begin{enumerate}
		\item Primary Semantic Analysis: Validates language semantics such as variable declarations and scope resolution.
		\item Preliminary Type Analysis: This phase performs initial type validations and checks. It includes tasks such as loading type definitions that might be referenced elsewhere in the program. These types need to be pre-loaded to ensure they are available and correctly resolved during subsequent stages of compilation.
		\item Primary Type Analysis: Conducts in-depth type validation and resolves type-related issues for the given statements and expressions.
	\end{enumerate}
\end{itemize}

\subsubsection{Target-Specific Requirement Checking}
\begin{itemize}
	\item Detailed explanation in section~\ref{sec:translation-to-the-ast}
	\item Performed by: Target Semantic Analyser
	\item Ensures compliance with the specific requirements of the target platform (JavaScript or TypeScript).
\end{itemize}

\subsubsection{Optimization}
\begin{itemize}
	\item Detailed explanation in section~\ref{sec:translation-to-the-ast}
	\item Performed by: Kipper Core Compiler
	\item Performs code optimizations, currently focused on treeshaking to eliminate unused code.
\end{itemize}

\subsubsection{Target-Specific Translation}
\begin{itemize}
\item Detailed explanation in section~\ref{sec:output-generation}
\item Performed by: Target Translator
\item Translates the optimized AST into the desired target language (JavaScript or TypeScript).
\end{itemize}

Each phase of the compiler is executed sequentially, with each step requiring the successful completion of the previous one. This ensures that each module of the compiler can safely rely on the correctness and completeness of the information provided by earlier steps. However, this approach limits the compiler's ability to recover from errors or detect all faults in a single execution. As a trade-off, this method simplifies the implementation and reduces overall complexity. Unlike TypeScript, for example, this means that the compiler cannot ignore certain errors and work around them. 

\section{Lexing \& Parsing}
\label{sec:lexing-parsing}
\setauthor{Luna Klatzer}

The first step in the compilation process is the lexing and parsing of the input program. This involves the tokenization of the program source code, where individual strings are classified into predefined categories, followed by syntactical analysis that organizes these tokens into statements, expressions, and declarations.

In the Kipper compiler, these two steps are carried out by the Kipper Lexer and Parser generated by \Gls{antlr4}, rather than being directly implemented within the compiler itself. These \Gls{antlr4}-generated components are constructed based on the predefined token and syntax rules specific to the Kipper language.

\subsection{Syntax definition}

The primary utility provided by \Gls{antlr4} lies in its ability to generate lexers and parsers automatically from an input file written in the Antlr4-specific ".g4" context-free grammar format. For Kipper, the lexer and parser each have distinct definitions, specifying the individual tokens and the rules for constructing the syntax tree that organizes and groups these tokens.

These definitions are created in a manner similar to other syntactical specification methods, such as \acrshort{bnf} (Backus-Naur Form), commonly used for context-free formal grammars. However, unlike \acrshort{bnf}, Antlr4 grammars have the added capability to include programmatic conditions and invocations, enabling more dynamic and adaptable grammar definitions.

\subsubsection{Lexer Grammar Definition}
\label{sec:lexer-grammar-definition}

In the case of the Kipper Lexer, it uses its own token definition file, which defines how characters are grouped into tokens. This file specifies the rules for identifying individual tokens while ignoring special characters that are not required for syntax analysis. Additionally, the lexer separates the matched tokens into various channels, allowing for more efficient handling and categorization during subsequent parsing steps. (see~\ref{sec:token-channels} for more detail)

For example, in Kipper, the lexer grammar includes constructs for identifying both comments and language-specific keywords. These definitions can be seen showcased below in listing~\ref{lst:kipper-lexer}.

\begin{lstlisting}[language=antlr4, caption={Sample snippet from Kipper Lexer grammar}, label={lst:kipper-lexer}]
	BlockComment : '/' .? '*/' -> channel(COMMENT) ;
	
	LineComment : '//' CommentContent -> channel(COMMENT) ;
	
	Pragma : '#pragma' CommentContent -> channel(PRAGMA) ;
	
	InstanceOf : 'instanceof';
	
	Const : 'const';
	
	Var : 'var';
\end{lstlisting}

In the grammar above, comments are directed to a separate channel using the \lstinline|-> channel| annotation. This helps isolate them from the main parsing flow while still retaining them for potential processing or documentation purposes. Pragmas, which are typically compiler directives, are handled similarly but redirected to a \lstinline|PRAGMA| channel.

\Gls{antlr4} grammars also include support for defining keywords, operators, and contextual language constructs. For instance, \lstinline|Const| and \lstinline|Var| are tokenized as reserved keywords, ensuring they are recognized unambiguously during lexical analysis. This explicit tokenization is critical for constructing a clear and predictable syntax tree, which serves as the foundation for the subsequent phases of interpretation and compilation.

\subsubsection{Syntactical Grammar Definition}
\label{sec:parser-grammar-definition}

Like the Kipper Lexer, the Kipper Parser is defined by its own syntax definition file. In this file, various rules are grouped into syntax rules that collectively form a hierarchical structure. The parser traverses this structure to determine which syntactical construct is represented by the individual tokens. These syntax rules are organized in a way that allows the parser to construct a parse tree by following specific paths, with each path representing a distinct syntactical structure in the Kipper language.

For example, a grammar rule defining a function declaration in Kipper could be expressed as seen in listing~\ref{lst:function-declaration}:

\begin{lstlisting}[language=antlr4, caption={Function Declaration Grammar}, label={lst:function-declaration}]
	functionDeclaration : 'def' declarator '(' parameterList? ')' '->' typeSpecifierExpression compoundStatement? ;
	
	parameterList : parameterDeclaration (',' parameterDeclaration)* ;
	
	parameterDeclaration : declarator ':' typeSpecifierExpression ;
\end{lstlisting}

In this grammar, the rule for \lstinline|functionDeclaration| begins with the keyword \lstinline|'def'|, followed by a \lstinline|declarator| (which represents the function name or identifier), and then an optional \lstinline|parameterList|. The rule includes a \lstinline|typeSpecifierExpression|, indicating the return type of the function, and optionally a \lstinline|compoundStatement|, which represents the function's body.

The \lstinline|parameterList| rule handles the optional inclusion of one or more parameters, each defined by \lstinline|parameterDeclaration|. Each \lstinline|parameterDeclaration| consists of a \lstinline|declarator| (the parameter's name) followed by a type specification. This structure clearly defines the expected syntax for a function declaration in Kipper, ensuring that the parser can accurately identify and process this construct.

Using these syntax rules, the Kipper parser can build a detailed parse tree, with nodes representing various language constructs, such as function definitions, parameters, and expressions. This hierarchical structure allows for efficient interpretation or compilation of Kipper code.

\subsection{Lexical analysis}

\subsubsection{Primary tokenisation}

The primary task of the lexical analysis is the tokenisation of the individual characters into grouped tokens which represent syntactical elements, such as identifiers, keywords, constant values, etc. Tokens serve as the building blocks for higher-level constructs in the parsing process, providing a simplified and structured representation of the raw source code.

The step of tokenisation is fairly straightforward as it simply follows the definitions provided in the grammar file (see~\ref{sec:lexer-grammar-definition}) and throws errors in case no associated token definition is found. Each token is assigned a specific type based on the grammar rules, ensuring that the source code adheres to the language's syntactical structure. If a character sequence does not match any defined rule, an error is raised, indicating the presence of invalid syntax.

The lexer operates as a state machine, scanning through the input character stream and categorizing sequences based on patterns defined in the grammar. These patterns may include regular expressions to match identifiers, numerical constants, or specific language keywords. Once tokens are identified they are put into the associated channels as explained in~\ref{sec:token-channels}.

\begin{figure}[h!]
	\centering
	\includegraphics[scale=1]{./pics/Lexer-Algorithm.drawio}
	\caption{The lexing process which categories the various tokens}
	\label{fig:implementation:Lexer-Algorithm}
\end{figure}

\subsubsection{Token Channels}
\label{sec:token-channels}

Next to the definition of the various tokens the grammar file also specifies what channel each token should be put into. These channels act as a stream of tokens where each stream represents different semantic parts of the program.

The channels which are implemented in the case of Kipper are:

\begin{itemize}
	\item \textbf{Default channel}
	\item \textbf{Comments channel}
	\item \textbf{Pragma channel}
	\item \textbf{Ignored channel}
\end{itemize}

The \textbf{default channel} serves as the primary stream, storing nearly all tokens in the program. It is the main channel used during the parsing step to construct a parse tree of the program.

The remaining channels are special-purpose streams that are excluded during parsing and cater to specific functionalities:

\begin{itemize}
	\item The \textbf{comments channel} is dedicated to storing all comments, which are logically irrelevant to the program and do not require parsing or further processing.
	\item The \textbf{pragma channel} contains compiler pragmasâ€”special instructions to the compiler that are processed independently from the standard syntax rules.
	\item The \textbf{ignored channel} is reserved for special characters that are significant only for token differentiation but have no logical relevance to the program, such as spaces. While spaces are critical for separating tokens, like with \lstinline|var x| and \lstinline|varx| they do not contribute to the program's logic and are therefore excluded from parsing.
\end{itemize}
	
\subsubsection{Nested Sub-Lexing}
\label{sec:nested-sub-lexing}

Besides standard sequential processing of the input, the Kipper Lexer also employs a technique called sub-lexing. Sub-lexing involves branching off the main lexing process and invoking a sub-lexer that operates under its own set of rules and guidelines. This specialized lexer handles specific subsets of tokens that require unique processing rules.

Sub-lexing is crucial for Kipper due to features such as templating, where code fragments are embedded within strings. In such cases, the lexer must correctly differentiate between string elements and code atoms (the inserted snippets inside the string). By using a simple push-and-pop mechanism, the lexers can function similarly to a stack, layering processing contexts on top of one another. Each context processes its corresponding string subset, enabling correct parsing of both regular syntax and embedded code within strings. This modular approach ensures cleaner handling of complex tokenisation scenarios and increases the lexer's flexibility.

\begin{figure}[h!]
	\centering
	\includegraphics[scale=0.85]{./pics/Sub-Lexer.drawio}
	\caption{The process of invoking a sub-lexer with the sample input \lstinline|f"Result: \{sample + 4\}"| (An example of a template string, or also format string, in the Kipper language), where all content between \lstinline|\{| and \lstinline|\}| is passed onto the sub-lexer.}
	\label{fig:implementation:sub-lexer}
\end{figure}

\subsection{Syntactic analysis}

\subsubsection{Primary syntactic analysis of the token stream}

With the lexer having already identified all tokens in a given program and ensured that only valid elements are present, the parser proceeds to analyse the program's structure and logic. This step is inherently more complex and often demands a significant amount of processing time. The complexity arises partly from the computational effort required to transform a token stream into a viable syntax tree and partly from the design of \Gls{antlr4}, which generates detailed context objects for each node in a program and as such requires a lot of memory and processing to call up each context.

The parser's primary task is to verify that the sequence of tokens conforms to the grammatical rules specified by the Kipper grammar. This involves checking whether constructs such as statements, expressions, and control structures are correctly formed.

\subsubsection{Building the parse tree}

Similar to the hierarchical structure defined by Kipper's grammar rules, the parse tree generated by the Kipper Parser also exhibits a hierarchical organization. Each parse node may have multiple child nodes and is connected to a single parent node, positioned syntactically one level higher within the tree. These nodes can either represent a rule node, such as \lstinline|expression|, or correspond directly to a simple lexer token.

The construction of a parse tree begins with a designated root node representing an entire file. From this root, branching occurs through intermediate rule nodes that capture various grammatical constructs, such as statements, definitions and expressions. These intermediate nodes eventually lead to leaf nodes, which are simple lexer tokens forming the smallest syntactic components of the program, such as identifiers, keywords, operators, and literals.

This hierarchical structure enables easy traversal and analysis of the program during later stages of compilation or interpretation. For instance, an arithmetic expression in a program, such as \lstinline|a + b * c|, would form a sub-tree where the root node corresponds to an expression rule, with child nodes representing the individual terms and operations in the correct precedence order.

A simplified representation of this tree structure is shown in figure~\ref{fig:implementation:parse-tree}.

\begin{figure}[h!]
	\centering
	\includegraphics[scale=0.95]{./pics/Parse-Tree.drawio}
	\caption{A simplified parse tree representation of the statement \lstinline|var x: num = 4;|.}
	\label{fig:implementation:parse-tree}
\end{figure}

\subsubsection{Programmatic conditions \& context-sensitive rules}

In addition to standard context-free rules, there are grammar rules that incorporate programmatic conditions, requiring specific requirements to be met beyond the standard syntactic structure. These rules are inherently context-sensitive, as they cannot be identified without considering the programâ€™s position and overall logic.

An example of a context-sensitive rule is the compound statement \lstinline|{ }|, which groups multiple statements and is typically used as the body of functions and methods. By definition, such statements are generally permitted only as top-level program nodes or as children of other statements, excluding cases where expressions such as lambda expressions specify one as a child. To accommodate lambdas having a structured body, two types of compound statements are defined. While they serve the same practical purpose, they are logically different due to their different contexts of usage: one as a child of another statement and another as a child of a lambda expression.

\subsection{AST (Abstract Syntax Tree)}
\label{sec:translation-to-the-ast}

The \acrshort{ast} represents a tree which groups together the most logically essential elements of a specific items and disregards all the other items not necessary in further processing. Lexer tokens, such as \lstinline|:|, \lstinline|=| or \lstinline|;| may be important syntactically as indicators for specific operations and structures, but once a specific parser rule kind has been determined and the meaning can be derived from that alone these tokens are not necessary anymore and can be discarded.

\subsubsection{Parse Tree Walking}

To transform the parse tree generated by the Kipper Parser, the compiler uses a tree-walking algorithm that systematically traverses the tree by entering and exiting grammar rules. During this process, the algorithm invokes specific handlers when they are defined for particular parse nodes. These handlers are designed to process only the most significant parse nodes, which typically correspond to essential syntactic constructs of the source code.

When a handler is called, it constructs a new \acrshort{ast} node and attaches it to the growing \acrshort{ast}. This selective processing approach ensures that irrelevant parse tree details, such as redundant intermediate nodes or syntactic artifacts, are automatically excluded from the \acrshort{ast}. The resulting \acrshort{ast} is a simplified and more abstract representation of the program's structure, capturing only the semantically relevant elements needed for subsequent stages of the compilation process.

The \acrshort{ast} generation result of such a tree walk process can be seen in figure~\ref{fig:implementation:ast}.

\begin{figure}[h!]
	\centering
	\includegraphics[scale=1]{./pics/AST.drawio}
	\caption{An AST produced by the statement \lstinline|var x: num = 4;|. The data in the brackets is in reality only defined and error checked during semantic analysis (see~\ref{sec:semantic-analysis}), but for the sake of clarity it is already provided here as to not cause confusion due to the missing metadata.}
	\label{fig:implementation:ast}
\end{figure}

\subsubsection{Utility provided by the AST}

In addition to providing a simplified abstraction of the original parse tree, individual \acrshort{ast} nodes also handle processing for several subsequent stages. Encapsulating these steps within a single class allows semantic analysis and type analysis to be efficiently layered and properly structured. This design is a core aspect of the compiler, as it centralizes data storage and ensures that the results of one processing stage directly influence subsequent stages.

For instance, if a node fails to successfully complete semantic analysis, the subsequent stages are automatically skipped. The node and any related parent structures are marked as faulty, allowing the compiler to handle errors gracefully without immediately crashing (see~\ref{sec:error-recovery} for a detailed explanation). Furthermore, the standardized and detailed structure of the \acrshort{ast} enables easy integration with other processing steps, as it stores or references all necessary information for specific parts of a program. This is particularly important during output generation (see~\ref{sec:output-generation}), where all existing information is required to accurately generate output code which logically adheres to the original program.

\section{Semantic Analysis}
\label{sec:semantic-analysis}
\setauthor{Luna Klatzer}

\section{Type Analysis}
\label{sec:type-analysis}
\setauthor{Luna Klatzer}

\section{Error recovery}
\label{sec:error-recovery}
\setauthor{Luna Klatzer}

The functionality of error recovery is integral to most modern compilers, as it allows the compiler to report multiple errors in a single compilation pass. Basic compilers often operate on an immediate fail-safe principle: upon encountering an error, the compilation process halts immediately to prevent the compiler from making incorrect assumptions about the program. While this approach ensures the integrity of the compilation process, it introduces a significant drawback. For large and complex programs, developers may need to repeatedly correct errors and recompile to uncover additional issues, leading to unnecessary delays and inefficiencies during the development process.

Given these issues, Kipper has implements its own error recovery algorithm into the semantic analysis which is able to recover from errors in given contexts and continue operation in following expressions or statements, which aren't associated with the original error.

\subsection{Error recovery algorithm}

\subsection{Special case: Syntax errors}

\section{Output Generation}
\label{sec:output-generation}
\setauthor{Lorenz Holzbauer}

\subsection{Introduction}

The Kipper compiler utilizes a modular architecture, allowing for the definition of custom targets and providing flexibility to accommodate various use cases. This modularity extends beyond basic configuration, enabling developers to specify target-specific features or behaviours. The modular design is achieved by dividing the compiler into two primary components: a frontend and a backend.

For comparison, the \acrshort{gcc} compiler achieves modularity by dividing its architecture into three components, as illustrated in Figure~\ref{fig:implementation:gcccompiler}. The frontend is responsible for verifying syntax and semantics, scanning the input, and performing type checking. It subsequently generates an intermediate representation (IR) of the code.

The middle-end then optimizes this intermediate representation, which is designed to be independent of the CPU target. Examples of middle-end optimizations include dead code elimination and detection of unreachable code. Finally, the backend takes the optimized intermediate code and generates target-dependent code. In the case of GCC, this involves generating assembly code.

\begin{figure}[h!]
	\centering
	\def\stackalignment{r}
	\stackunder{\includegraphics[scale=0.36]{./pics/Compiler_design}}{\scriptsize Source: \href{https://commons.wikimedia.org/wiki/File:Compiler_design.svg}{https://commons.wikimedia.org}}
	\caption{The design of the GCC compiler}
	\label{fig:implementation:gcccompiler}
\end{figure}

As of now, Kipper generates either TypeScript or JavaScript code as its output. The target language is specified in the Kipper CLI utility using the flag --target={js|ts}. If this flag is not provided, the default target is JavaScript.

Currently, Kipper does not include a middle-end component. This decision was made to avoid the additional complexity associated with implementing a full middle-end, as the resulting performance improvements in generated code would be only marginal. Instead, the frontend directly passes the \acrshort{ast} (Abstract Syntax Tree) to the backend. While Kipper does perform some post-analysis optimizations, these are presently limited to tree-shaking.

\subsection{Role of the AST in the output generation}

Kipper generally uses an \acrshort{ast} as the primary representation of the code from the input program. The \acrshort{ast} serves as a hierarchical structure that represents the program's source code. The root node of the \acrshort{ast} corresponds to the entire program, while each child node represents a specific construct such as a statement, scope, or other language feature.

Each node in the \acrshort{ast} contains the semantic data and type-related information of the corresponding statement, as well as a string representation of the original parser node. Additionally, every node includes a kind identification propertyâ€”a unique, hard-coded number in the compilerâ€”to uniquely identify the type of the node. This property aids in determining the type of construct, such as a declaration, an expression, or a statement.

Every node also maintains a list of child nodes, representing nested or dependent components of the construct. Once the \acrshort{ast} is fully constructed, it is wrapped and passed to the code generator for further processing.

\subsection{Algorithms used for Output Generation}

There is a plethora of algorithms available to generate code in the target language. They can be classified by their input data structure. Some algorithms need a tree-like \acrshort{ir}, others need a linear \acrshort{ir} structure.

\subsubsection{Linear Algorithms}

Linear algorithms are commonly employed when compiling high-level languages to machine code or bytecode. These algorithms treat the input as a flat, ordered sequence and process it sequentially. Intermediate representations (IR) are typically in the form of three-address code or static-single-assignment (SSA) form. Linear algorithms process the code one instruction at a time in sequence. Upon completion of an instruction, it is added to a list, which is eventually concatenated and written to the output file.

Three-address code consists of three operands and typically represents an assignment with a binary operator~\cite{wiki:threeaddress}. An instruction may have up to three operands, although fewer can also be used. In listing~\ref{lst:implementation:threeaddresscode}, the problem is divided into multiple instructions. This structure allows the compiler to easily translate the instructions into assembly language or bytecode, which share a similar format. Additionally, the compiler can identify unused code by determining whether a variable is referenced later in the code.

\begin{lstlisting}[language=TypeScript,caption=Three-address code,label=lst:implementation:threeaddresscode]
// Problem
x = (-b + sqrt(b^2 - 4*a*c)) / (2*a)

// Solution
t1 := b * b
t2 := 4 * a
t3 := t2 * c
t4 := t1 - t3
t5 := sqrt(t4)
t6 := 0 - b
t7 := t5 + t6
t8 := 2 * a
t9 := t7 / t8
x := t9
\end{lstlisting}

The Static Single-Assignment (SSA) form is an alternative intermediate representation in which each variable is assigned exactly once~\cite{wiki:singlestatic}. It is widely used in compilers such as \acrshort{gcc}. The primary advantage of SSA is that it simplifies the code and enhances the effectiveness of compiler optimizations.

In listing~\ref{lst:implementation:staticsingleassignmentform}, a variable \lstinline|y| is assigned twice. The first assignment is redundant. In SSA form, the compiler can identify that the assignment to \lstinline|y1| is unnecessary, as it is not used in the subsequent code. Optimizations improved by the use of SSA include dead-code elimination, constant propagation, and register allocation.

\begin{lstlisting}[language=TypeScript,caption=Static single-assignment form,label=lst:implementation:staticsingleassignmentform]
// Problem
y := 1
y := 2
x := y

// Solution
y1 := 1
y2 := 2
x1 := y2
\end{lstlisting}

\subsubsection{Tree-based Algorithms}

Tree-based algorithms are commonly employed in transpilers, as they allow the code to remain human-readable while preserving the general structure of the source code. This ensures that the structure of scopes and statements is maintained. Consequently, the components of the code are represented as nodes in a tree-like structure. Kipper adopts a bottom-up code generation algorithm, where a tree-walker recursively traverses the tree and generates the output starting from the most deeply nested node. This method was chosen for its ease of visualization and implementation, as well as the desire to retain human-readable and extendable source code without aggressive optimizations or transformations. This is not achievable with linear algorithms, which transform the code into either three-address code or static single-assignment form, leading to a loss of important contextual and structural information.

Tree-based code generation can be used to generate bytecode, which is then optimized and further compiled by a linear algorithm. However, tree-based algorithms have the limitation of enabling only local optimizations within the respective nodes. Additionally, they can be computationally expensive, particularly when dealing with complex input, due to the recursive nature of the tree-walking process. As a result, tree-based designs are not commonly employed in bytecode-generating compilers. However, \acrshort{gcc} uses a tree-based design in two of its language-independent IRs: GIMPLE and GENERIC~\cite{gcc:gimpletuples}.

\subsection{Generation Algorithm}

The output generation process in Kipper begins by setting up the target environment and generating any necessary requirements, as detailed in section~\ref{sec:requirements}. After completing the setup, the compiler iterates over the previously generated \acrshort{ast} nodes by invoking the \lstinline|translateCtxAndChildren| function for each node. This function recursively traverses the tree, generating code for each child node. Each node returns a string representing its output code, which is subsequently processed by its parent node. The process continues until the root node aggregates all generated strings and passes the final merged output to a function responsible for writing the code to a file.

This bottom-up processing approach ensures that the translation of child nodes is completed before their corresponding parent nodes are processed. Consequently, parent nodes can extract and integrate any required information from their child nodes. This methodology is particularly critical for complex structures, as these often depend on the embedded code generated by their children. The generated code is conveyed to the parent as an array of tokens, which represent the textual form of the output code.

The output generation process guarantees reliability because the \acrshort{ast} has already undergone validation for syntactic and semantic correctness during earlier compilation phases.

The implementation of this translation algorithm is presented in figure~\ref{fig:implementation:translationalgorithm}.

\begin{figure}[h!]
	\centering
	\includegraphics[scale=0.9]{./pics/Output-Generation.drawio}
	\caption{A simplified version of the tree-walker generation algorithm.}
	\label{fig:implementation:translationalgorithm}
\end{figure}

The code generation function of a node takes the node as an argument and retrieves its semantic and type-semantic data. This information is then used to translate the node's children, which are properties defined within the semantic data, into source code. The generated code fragments are concatenated into a single string array and returned. This process is demonstrated in listing~\ref{lst:implementation:instanceofgeneration}, where the translation of an \lstinline|instanceOf| expression is shown as an example.

\begin{lstlisting}[language=TypeScript,caption=The code generation function of a \lstinline|instanceOf| expression,label=lst:implementation:instanceofgeneration]
instanceOfExpression = async (node: InstanceOfExpression): ... => {
	const semanticData = node.getSemanticData();
	const typeData = node.getTypeSemanticData();
	const operand = await semanticData.operand.translateCtxAndChildren();
	const classType = TargetJS.getRuntimeType(typeData.classType);

    return [...operand, " ", "instanceof", " ", classType];
  };
\end{lstlisting}

The code generator functions for the individual nodes are implemented in the code generator class \lstinline|JavaScriptTargetCodeGenerator|. Due to the similarity between TypeScript and JavaScript, the TypeScript code generator extends the JavaScript code generator and overrides the functions that are unique to TypeScript. This eliminates duplicate code fragments.

\subsection{Target Requirements Generation}
\label{sec:requirements}

Kipper is designed to have a runtime that is as small as possible while still having all the needed functionality bundled in it. This means, that the compiler should only include functions and objects into the runtime, that are needed by the user. This aligns with our goal of keeping the compiler modular and minimal. Due to the removal of unused components in a process called "tree-shaking", the output code is kept small and efficient.

Kipper includes a range of built-in functions and features to support its runtime environment. These built-ins are organized and managed using a scoped approach. The global scope contains core runtime features that are always required, such as basic type handling and error reporting. Beyond the global scope, additional features are selectively included based on the specific requirements of the program being compiled.

\subsubsection{Conditional features}

Conditional features are runtime components that are included only when explicitly required by the program being compiled. These features can range from commonly used operations like match and slice to more specialized or program-specific utilities.

Unlike essential runtime components housed in the global scope, conditional features are added selectively based on an analysis of the program's structure and functionality. The decision to include conditional features occurs during the requirements generation phase. 

As the compiler traverses the \acrshort{ast}, it examines each node to determine whether a specific built-in function or runtime operation is invoked. If a feature like slice is used in the source code, it is flagged as necessary and included in the consolidated requirements list. This ensures that only the relevant components are integrated into the final runtime environment.

An example of a conditional feature would be the \lstinline|slice| function as shown in listing~\ref{lst:implementation:slicefunction}. 

\begin{lstlisting}[language=TypeScript,caption=The Slice Operation,label=lst:implementation:slicefunction]
var valid: str = "321";
print(valid[1:2]); // 2
\end{lstlisting}

This function takes an array as input and extracts a section starting from the index the first argument provides and ending at the index the second argument provides.

When the compiler encounters the slice operator, it registers the function as needed and therefore includes it into the output code. This process can be seen in listing~\ref{lst:implementation:sliceinternal}. 

The compiler calls the slice function in the \lstinline|BuiltInGenerator|. The abstract \lstinline|BuiltInGenerator| class is a collection of functions responsible for generating the code for the built-in functions in a specific target language. This means that each target language must have an individual implementation for each built-in function of Kipper within its output generator, which subsequently adds the corresponding code into the resulting program.

The generator in the listing~\ref{lst:implementation:sliceinternal} generates the required JavaScript code for the function by utilizing the built-in \lstinline|slice| function provided by standard JavaScript.

\begin{lstlisting}[language=TypeScript,caption=Slice in the JavaScript BuiltInGenerator,label=lst:implementation:sliceinternal]
async slice(funcSpec: InternalFunction): Promise<Array<TranslatedCodeLine>> {
	const signature = getJSFunctionSignature(funcSpec);
	const objLikeIdentifier = signature.params[0];
	const startIdentifier = signature.params[1];
	const endIdentifier = signature.params[2];

	return genJSFunction(
		signature,
		`{ return ${objLikeIdentifier} ? ${objLikeIdentifier}.slice(${startIdentifier}, ${endIdentifier}) : ${objLikeIdentifier}; }`,
	);
  }
\end{lstlisting}

The \lstinline|slice| function generated by the code generator as shown in listing~\ref{lst:implementation:sliceinternal} is represented in listing~\ref{lst:implementation:slicegenerated}. This function will be executed at runtime and perform the required operation, in this case slicing the given argument.

\begin{lstlisting}[language=TypeScript,caption=Slice in the target language TypeScript,label=lst:implementation:slicegenerated]
slice: function slice<T>(objLike: T, start: number | undefined, end: number | undefined): T {
	return objLike ? objLike.slice(start, end) : objLike;
},
\end{lstlisting}

\subsubsection{The global scope}

The global scope in programming represents the top-level execution context where variables, functions, and objects are accessible throughout the entire runtime environment unless explicitly restricted. In JavaScript, the global scope is particularly significant as it varies across runtime environments like browsers, Node.js, and Web Workers. This variability necessitates robust mechanisms for identifying and managing the global context to ensure compatibility across environments.

For example, JavaScript defines several global objects, such as window in browsers, global in Node.js, and self in Web Workers. Modern JavaScript unifies these under \lstinline|__globalScope|, a standardized global object that provides a consistent way to access the global scope regardless of the environment. However, not all environments support \lstinline|__globalScope|, which is why fallback mechanisms are often used.

The Kipper global scope contains all the runtime features that are required. It is important, that the global scope exists only once, therefore the program needs to check at runtime, if the scope already exists. This can be seen in listing~\ref{lst:implementation:globalscopelogic}. It first checks if \lstinline|__globalScope| is already defined and uses it if available. If not, it attempts to use \lstinline|globalThis|, the modern standard JavaScript global object. If \lstinline|globalThis| is not defined, it checks for window in browser environments, global in Node.js, or self in Web Workers. If none of these are defined, it falls back to an empty object. This ensures that the \lstinline|__globalScope| variable is always initialized, regardless of the environment, allowing consistent and safe access to the global scope.

\begin{lstlisting}[language=TypeScript,caption=Global Scope Logic,label=lst:implementation:globalscopelogic]
var __globalScope = typeof __globalScope !== "undefined" ? __globalScope :
    typeof globalThis !== "undefined" ? globalThis :
    typeof window !== "undefined" ? window :
    typeof global !== "undefined" ? global :
    typeof self !== "undefined" ? self : {};
\end{lstlisting}

\subsubsection{Internal functions}

Internal functions in Kipper serve as an essential part of the runtime, yet they are designed to remain hidden from the user-facing API. These functions provide support for various runtime operations and compiler processes but are not directly accessible or callable in the user's program. This ensures that the runtime environment remains clean and minimal while still delivering the required functionality.

An example of an internal function would be the \lstinline|assignTypeMeta| function, which adds metadata to a runtime type. This is useful for runtime type comparison and described in detail in chapter~\ref{subsec:builtintypes}. This function never gets exposed to the user but is called internally when an interface gets declared.

A key characteristic of internal functions is their dynamic inclusion in the runtime environment. During the requirements generation phase, the compiler identifies whether a program's functionality depends on any internal mechanisms. If so, the corresponding internal functions are included in the runtime.

\subsubsection{Requirements Generation}

The requirements generation process in Kipper begins during the compilation phase. As the compiler traverses the \acrshort{ast}, it analyses the nodes to determine the features needed by the program. This analysis produces a set of requirements, which are then used to configure the runtime environment. This works by using feature registration. If an AST node needs a certain feature, the reference is added to the program context by using the \lstinline|this.programCtx.addInternalReference| function. When a feature is added more once, all further additions get ignored, as the function is already available. After all the features are registered, the target generates the source code in the required language and inserts it into the output code.

\subsection{Differences between the Target Languages}

The implementation of a compiler or transpiler targeting multiple programming languages often requires handling the specific quirks and requirements of each target. As Kipper is a web development language, we target both TypeScript and JavaScript. Both languages share a common foundation but diverge significantly in their syntax rules, semantics, and type systems.

One of the primary challenges in supporting both JavaScript and TypeScript as target languages is their differing treatment of identifiers, reserved keywords, and type declarations. While JavaScript is dynamically typed and relatively permissive in terms of variable naming and usage, TypeScript enforces a stricter set of rules due to its static type-checking capabilities.

\subsubsection{Reserved Keywords}

Both JavaScript and TypeScript have a set of reserved keywords that cannot be used as identifiers. However, TypeScript introduces additional constraints by reserving type-related keywords, which are not present in JavaScript. For instance, class is a reserved keyword in both languages and cannot be used as a variable name. In contrast, TypeScript also reserves names like let, number, and other type names, making them invalid as variable or function names.

Kipper handles these reserved keywords by checking for them at compile time. The compiler compares against a hard-coded list of keywords and in case it finds one, it throws an \lstinline|ReservedIdentifierOverwriteError|. This list of keywords contains both the reserved words of JavaScript and TypeScript, as this minimizes redundancy and complexity. In addition to that, it also forces the developer to use sensible variable names, as JavaScript is quite lenient with it's reserved keywords. listing~\ref{lst:implementation:reservedkeywords} illustrates this.

\begin{lstlisting}[language=TypeScript,caption=Reserved Keywords in TS and JS,label=lst:implementation:reservedkeywords]
// Invalid in TypeScript
let let = 5;  // Error: Cannot use 'let' as an identifier
let number = 10;  // Error: Cannot use 'number' as an identifier

// Valid in JavaScript
var let = 5;  // No error
var number = 10;  // No error
\end{lstlisting}

\subsubsection{Type Annotations}

TypeScript introduces type annotations as part of its static type system. This means that while generating the TypeScript output, Kipper has to append type information in variable assignments, functions and lambdas. This works by overriding the JavaScript implementation of the code generator function and converting the AST-internal type of the node to a TypeScript type. 

Figures~\ref{fig:implementation:kipper-to-javascript-translation-example} and~\ref{fig:implementation:kipper-to-typescript-translation-example} shows the difference between the JavaScript code generator function and the TypeScript code generator function for variable assignments with the source example \lstinline|var x: num = 5;| being translated. In the code block that generates TypeScript, there are additional code tokens after the storage and the identifier that insert the type of the object into the output code.

\begin{figure}[h!]
	\centering
	\includegraphics[scale=1.1]{./pics/Kipper-to-JavaScript-Translation-Example}
	\caption{The simplified translation process of the example \lstinline|var x: num = 5;| into JavaScript.}
	\label{fig:implementation:kipper-to-javascript-translation-example}
\end{figure}

\begin{figure}[h!]
	\centering
	\includegraphics[scale=1.1]{./pics/Kipper-to-TypeScript-Translation-Example}
	\caption{The simplified translation process of the example \lstinline|var x: num = 5;| into TypeScript.}
	\label{fig:implementation:kipper-to-typescript-translation-example}
\end{figure}

Other code generators such as the ones for function declarations and lambdas behave similarly when taking type annotations into account.

\subsection{Stylistic Choices}

The syntax of Kipper is specifically designed to ease the transition of existing TypeScript and JavaScript developers to Kipper. Therefore it was important, to keep the output as similar to these languages as possible. We achieved this by adhering to the following principles.

\subsubsection{Human readable output}

The primary goal of Kipper's output generation is to produce code that mirrors human-written TS or JS as closely as possible. To achieve this, Kipper avoids unnecessary abstractions or layers that could obscure the intent of the code. For instance, variable names and function identifiers are preserved during \gls{transpilation} without introducing machine-generated names or hashing schemes. This contrasts with languages like CoffeeScript, where the output, while functional, often requires familiarity with the transpiler's conventions to interpret effectively.

\subsubsection{No Code Compression}

Kipper explicitly avoids code compression techniques such as minification or inlining that can hinder readability. While compression is useful in production environments to reduce payload size, it is opposed to the goals of Kipper, as it negatively impacts code readability.

\subsubsection{Standardized Style Format}

Kipper enforces a standardized style format for its output to ensure consistency and predictability. It uses two spaces for block-level indentation to maintain clarity and avoid confusion with tab-based formatting. Braces are explicitly used for block delimiters, and semicolons terminate statements, adhering to common TypeScript/JavaScript conventions.

\subsubsection{Scope Visibility}

A critical aspect of Kipper's design is the clear representation of scopes in the generated output. Kipper employs explicit declaration keywords such as \lstinline|let|, \lstinline|const|, and \lstinline|function| to discriminate variable and function scopes. Indentation and brace placement further enhance the visual hierarchy, making it easy to identify nested scopes and understand their boundaries. This approach contrasts with languages like Python, where indentation alone determines scope, or languages like Lua, where scope visibility may rely on implicit conventions.

\subsubsection{Editable Code}

One of Kipper's unique selling points is that its transpiled output is not just readable but also editable. Developers can treat the generated TS/JS code as if it were written manually, enabling seamless integration with existing projects. By making the output editable, Kipper empowers developers to tailor the transpiled code to their specific needs without relying solely on the original Kipper source.

\subsubsection{Comparison with other languages}

CoffeeScript aimed to simplify JavaScript syntax but often produced output that was hard to debug due to its reliance on non-standard conventions. Kipper avoids these pitfalls by aligning its syntax and output with established TypeScript/JavaScript practices. While TypeScript generates clean and maintainable JavaScript, it requires a compilation step that may introduce additional complexity. Kipper simplifies this process by direct \gls{transpilation} to both TypeScript and JavaScript, offering flexibility without sacrificing readability. Babel's output is highly optimized for compatibility but can be dense and difficult to modify. Kipper prioritizes maintainability over optimization, ensuring that the output remains approachable for developers.

\section{Integrated Runtime}
\label{sec:integrated-runtime}
\setauthor{Lorenz Holzbauer}

\subsection{Runtime Type implementations in other languages}
\label{chap:runtime-other-languages}

\subsubsection{Nominal Type Systems}

Nominal type systems are used in most modern object-orientated programming languages like Java and C\#. In these systems, types are identified by their unique names and can only be assigned to themselves. Additionally, two types are considered compatible, if one type is a subtype of the other one, as can bee seen in listing~\ref{lst:implementation:javanominaltyping}. Here a  \lstinline|Programmer| is an \lstinline|Employee|, but not the other way around. This means \lstinline|Programmer| instances have all the properties and methods an \lstinline|Employee| has while also having additional ones specific to \lstinline|Programmer|. The relationships are as such inherited, so \lstinline|SeniorDeveloper| is still an \lstinline|Employee| and a \lstinline|Programmer| at the same time. Even though the \lstinline|SeniorDeveloper| adds no new functionality to the \lstinline|Programmer|, it is not treated the same. Nominal typing improves code readability and maintainability, due to the explicit inheritance declaration. On the other hand, this increases code redundancy for similar or even identical but not related structures.

\begin{lstlisting}[language=Java,caption=Example of nominal typing in Java,label=lst:implementation:javanominaltyping]
class Employee {
	public float salary;
}

class Programmer extends Employee {
	public float bonus;
}

class SeniorDeveloper extends Programmer { }
\end{lstlisting}

\subsubsection{Structural Type Systems}

Structural type systems compare types by their structure. This means, if two differently named types have the same properties and methods, then they are the same type. An example of this would be OCaml, with its object subsystem being typed this way. Classes in OCaml only serve as functions for creating objects. In listing~\ref{lst:implementation:ocamlstructuraltyping} there is a function that requires a function \lstinline|speak| returning the type \lstinline|string|. Both the \lstinline|dog| object as well as the \lstinline|cat| object fulfill this condition, therefore both are treated equal. Most importantly, these compatibility checks happen at compile time, as OCaml is a static language. Structural typing allows for a lot of flexibility as it promotes code reuse. Furthermore it avoids explicit inheritance hierarchies.

\begin{lstlisting}[language=caml,caption=Example of structural typing in Ocaml,label=lst:implementation:ocamlstructuraltyping]
	let make_speak (obj : < speak : string >) =
	obj#speak

	let dog = object
	method speak = "Woof!"
	end

	let cat = object
	method speak = "Meow!"
	end

	let () =
	print_endline (make_speak dog);
	print_endline (make_speak cat);
\end{lstlisting}

\subsubsection{Duck Typed Systems - Duck Typing}

Duck Typing is the usage of a structural type system in dynamic languages. It is the practical application of the "Duck Test", therefore if it quacks like a duck, and walks like a duck, then it must be a duck. In programming languages this means that if an object has all methods and properties required by a type, then it is of that type. The most prominent language utilizing Duck Typing is TypeScript. As can be seen in listing~\ref{lst:implementation:javascriptducktyping}, the \lstinline|duck| and the \lstinline|person| have the same methods and properties, henceforth they are of the same type. The \lstinline|dog| object on the other hand does not implement the \lstinline|quack|function, which equates to not being a \lstinline|duck|. Duck typing simplifies the code by removing type constraints, while still encouraging polymorphism without complex inheritance.

\begin{lstlisting}[language=Typescript,caption=Example of duck typing in TypeScript,label=lst:implementation:javascriptducktyping]
interface Duck {
	quack(): void;
}

const duck: Duck = {
	quack: function () {
		console.log("Quack!");
	}
};

const person: Duck = {
	quack: function () {
		console.log("I am a person but I can quack!");
	}
};

const dog: Duck = {
	bark: function () {
		console.log("Woof!");
	}
}; // <- causes an error in the static type checker
\end{lstlisting}

Given that duck typing allows dynamic data to be easily checked and assigned to any interface, Kipper adopts a similar system to that of TypeScript but introduces notable differences in how interfaces behave and how dynamic data is handled. For instance, casting an \lstinline|any| object to an interface in Kipper will result in a runtime error if the object does not possess all the required members. In contrast, TypeScript permits such an operation without performing any type checks at runtime.

\subsection{Runtime Type Concept in Kipper}

As previously explained (see section~\ref{sec:type-system}) the Kipper programming language utilises a similar type system to TypeScript with static typing and a duck-typing approach to complex data and \acrshort{oop} structures. However, unlike TypeScript, we want to ensure full type safety at a runtime level and force the developer to specify the required types and handle edge cases, such as casts and type inference.

Using this approach Kipper allows untyped values, as is the case with the \lstinline|any| type that is often returned by requests or web elements, or dynamic values to be compared with types that have clearly defined boundaries, such as primitives, arrays, functions, classes, and interfaces, removing any ambiguities that could cause errors.

To allow this functionality, during code generation all user-defined interfaces are converted into runtime types that store the information needed to perform type checks, which form the basis of casts and strict type safety. These are then utilised alongside the built-in runtime types, such as \lstinline|num|, \lstinline|str| or \lstinline|obj|, to enable the compiler to add necessary checks and runtime references to any cast, match or typeof operation, guaranteeing that they are fully type-safe.

With the exception of interfaces, classes, and generics, types are primarily distinguished by their names. In these cases, type equality checks are performed using nominal comparisons, where the name acts as a unique identifier within the given scope e.g. type \lstinline|num| is only assignable to \lstinline|num|. For more complex structures, additional informationâ€”such as members or generic parametersâ€”is also considered.

In the case of interfaces, the names and types of fields and methods are used as discriminators. These fields and methods represent the minimum blueprint that an object must implement to be considered compatible with the interface and thus "assignable". In this regard, Kipper adopts the same duck-typing approach found in TypeScript.

For generics, which include \lstinline|Array<T>| and \lstinline|Func<T..., R>|, the identifier is used alongside the provided generic parameters to determine assignability. This ensures that when one generic is assigned to another, all parameters must match. For instance, \lstinline|Array<num>| cannot be assigned to \lstinline|Array<str>"|and vice versa, even if their overall structure is identical.

For user-defined classes, the compiler relies on the prototype to serve as the discriminator. In practice, this behaviour is similar to that of primitives, as different classes cannot be assigned to each other.

\subsection{Base Type for the Kipper Runtime}
\label{subsec:basetype}

In practice, all user-defined and built-in types inherit from a basic \lstinline|KipperType| class in the runtime environment. This class is a simple blueprint of what a type could do and what forms a type may take on. A simple version of such a class can be seen in listing~\ref{lst:implementation:runtimetypestructure}.

\begin{lstlisting}[language=TypeScript,caption=The structure of a runtime type,label=lst:implementation:runtimetypestructure]
	class KipperType {
		constructor(name, fields, methods, baseType = undefined, customComparer = undefined) {
			this.name = name;
			this.fields = fields;
			this.methods = methods;
			this.baseType = baseType;
			this.customComparer = customComparer;
		}

		accepts(obj) {
			if (this === obj) return true;
			return obj instanceof KipperType && this.customComparer ? this.customComparer(this, obj) : false;
		}
	}
\end{lstlisting}

As already mentioned types primarily rely on identifier checks to differentiate themselves from other types. Given though that there are slight differences in how types operate, they generally define themselves with what they are compatible using a comparator function. This comparator is already predefined for all built-ins in the runtime library and any user structures build on top of the existing rules established in the library.

Type \lstinline|any| is an exception and is the only type that accepts any value you provide. However, assigning "any" to anything other than \lstinline|any| is forbidden and it is necessary to cast it to a different type in order to use the stored value. By  \lstinline|any| is as useless as possible, in order to force the developer into typechecking it.

Furthermore, classes are also exempt from this comparator behaviour, as classes behave like a value during runtime and provide a prototype which can simply be used to check if an object is an instance of that class.

\subsection{Built-in Types for the Kipper Runtime}
\label{subsec:builtintypes}

Built-in runtime types serve as the foundation of the type system and make up the parts of more complex constructs like interfaces. Built-in runtime types are compared at runtime by comparing their references, as they are uniquely defined at the start of the output code and available in the global scope. The implementations of such structures can be seen in listing~\ref{lst:implementation:builtinruntimetypes}.

\begin{lstlisting}[language=TypeScript,caption=Examples for the built-in runtime types,label=lst:implementation:builtinruntimetypes]
	const __type_any =
	new KipperType("any", undefined, undefined);

	const __type_undefined =
	new KipperType("undefined", undefined, undefined, undefined, (a, b) => a.name === b.name);

	const __type_str =
	new KipperType("str", undefined, undefined, undefined, (a, b) => a.name === b.name);
\end{lstlisting}

In addition to the core primitive typesâ€”such as \lstinline|bool|, \lstinline|str|, \lstinline|num|, and othersâ€”there are built-in implementations for generic types, including \lstinline|Array<T>| and  \lstinline|Func<T..., R>|. These additionally define their generic parameters which generally default to a standard \lstinline|any| type as can be seen in listing~\ref{lst:implementation:genericbuiltintypes}.

\begin{lstlisting}[language=Typescript,caption=Generic built-in types,label=lst:implementation:genericbuiltintypes]
const __type_Array = new KipperGenericType("Array", undefined, undefined, {T: __type_any});
const __type_Func = new KipperGenericType("Func", undefined, undefined, {T: [], R: __type_any});
\end{lstlisting}

As can be seen in listing~\ref{lst:implementation:genericbuiltintypes}, generic types are implemented using a special \lstinline|KipperGenericType| class. This class, shown in listing~\ref{lst:implementation:generickippertype}, extends the \lstinline|KipperType| and includes an additional field for generic arguments. Most importantly, it includes the method \lstinline|changeGenericTypeArguments|. which allows for modifying a type's generic arguments at runtime. It is used in lambda and array definitions, where the built-in generic runtime type is used and then modified to represent the specified generic parameters. When for example an array is initialized, it first gets assigned the default \lstinline|Array<any>| runtime type, which is then modified by the  \lstinline|changeGenericTypeArguments| method to create the required type, such  \lstinline|Array<num>|. Arrays for example use the specified type for their elements, whilst functions require a return type as well as an array of argument types. The \lstinline|Func<T..., R>| type on the other hand is used by lambda definitions, which are user-defined functions with a specific return type and arguments without a name \ref{lst:implementation:generickippertype}.

\begin{lstlisting}[language=Typescript,caption=Generic Kipper Type,label=lst:implementation:generickippertype]
class KipperGenericType extends KipperType {
	constructor(name, fields, methods, genericArgs, baseType = null) {
		super(name, fields, methods, baseType);
		this.genericArgs = genericArgs;
	}
	isCompatibleWith(obj) {
		return this.name === obj.name;
	}
	changeGenericTypeArguments(genericArgs) {
		return new KipperGenericType(
		this.name,
		this.fields,
		this.methods,
		genericArgs,
		this.baseType
		);
	}
}
\end{lstlisting}

\subsection{Runtime Errors}

Other built-ins include error classes, which are used in the error handling system to represent runtime errors caused by invalid user operations. The base \lstinline|KipperError| type has a name property and extends the target language's error type as can be seen in listing~\ref{lst:implementation:kippererrortypes}. Additional error types inherit this base type and extend it with additional error information. For instance, the  \lstinline|KipperIndexError| is used whenever an index was out of bounds.

\begin{lstlisting}[language=Typescript,caption=Kipper error types,label=lst:implementation:kippererrortypes]
class KipperError extends Error {
	constructor(msg) {
		super(msg);
		this.name = "KipError";
	}
}

class KipperIndexError extends KipperError {
	constructor(msg) { 
		super(msg); 
		this.name = 'KipIndexError'; 
	} 
}
\end{lstlisting}

\subsection{Runtime Generation for Interfaces}

Unlike TypeScript, in Kipper all interfaces possess a runtime counterpart, which stores all the required information to verify type compatibility during runtime. This process is managed by the Kipper code generator, which adds custom type instances to the compiled code that represent the structures of the user-defined interfaces with all its methods and properties including their respective types.

Now take for example the given interfaces presented in listing~\ref{lst:implementation:inputinterface-car} and~\ref{lst:implementation:inputinterface-person}.

\begin{lstlisting}[language=Typescript,caption=Example interface \lstinline|Car| in the Kipper language,label=lst:implementation:inputinterface-car]
interface Car {
	brand: str;
	honk(volume: num): void;
	year: num;
}
\end{lstlisting}

\begin{lstlisting}[language=Typescript,caption=Example interface \lstinline|Person| in the Kipper language including a reference to a different interface,label=lst:implementation:inputinterface-person]
interface Person {
	name: str;
	age: num;
	car: Car;
}
\end{lstlisting}

At compile time, the generator function iterates over the interface's members and differentiates between properties and methods. The function keeps separate lists of already generated runtime representations for properties and methods.

If it detects a property, the type and semantic data of the given property is extracted. When the property's type is a built-in type, the respective runtime type already provided by the Kipper runtime library is used. If not, we can assume the property's type is a reference to another type structure, which will be simply referenced in our new type structure. This data is stored in an instance of \lstinline|__kipper.Property|, which is finally added to the list of properties in the interface.

In case a method is detected, the generator function fetches the return type and the method's name. If the method has any arguments, the name and type of each argument also gets evaluated and then included in the definition of the \lstinline|__kipper.Method|. After that, it gets added to the interface as well and is stored in its own separate method list.

Translating the interface \lstinline|Car| shown in listing~\ref{lst:implementation:inputinterface-car} would result in an output runtime code identical to that in listing~\ref{lst:implementation:runtimeinterface-car} and translating \lstinline|Person| would result in the code as presented in listing~\ref{lst:implementation:runtimeinterface-person}.

\begin{lstlisting}[language=Typescript,caption=The runtime representation of the example interface \lstinline|Car|,label=lst:implementation:runtimeinterface-car]
const __intf_Car = new __kipper.Type(
	"Car",
	[
		new __kipper.Property("brand", __kipper.builtIn.str),
		new __kipper.Property("year", __kipper.builtIn.num),
	],
	[
		new __kipper.Method("honk", __kipper.builtIn.void,
			[
				new __kipper.Property("volume", __kipper.builtIn.num),
			]
		),
	]
);
\end{lstlisting}

\begin{lstlisting}[language=Typescript,caption=The runtime representation of the example interface \lstinline|Person|,label=lst:implementation:runtimeinterface-person]
const __intf_Person = new __kipper.Type(
	"Person",
	[
		new __kipper.Property("name", __kipper.builtIn.str),
		new __kipper.Property("age", __kipper.builtIn.num),
		new __kipper.Property("car", __intf_Car),
	],
	[]
);
\end{lstlisting}

As shown in listing~\ref{lst:implementation:runtimeinterface-car} and listing~\ref{lst:implementation:runtimeinterface-person}, the properties and methods of an interface are encapsulated within a \lstinline|KipperType| instance, identified by the  \lstinline|__intf_| prefix. The code for this runtime interface is included directly in the output file, where it can be accessed by any functionality that requires it. To reference the generated interface, the compiler maintains a symbol table that tracks all defined interfaces. The code generator then inserts runtime references to these interfaces wherever necessary.

Notable usages for runtime type-checking include the \lstinline|matches| operator (see section \ref{subsec:matches}) and the \lstinline|typeof| operator (see section \ref{subsec:typeof}).

\subsection{Matches Operator for Interfaces}
\label{subsec:matches}

There are multiple approaches for comparing objects at runtime. One method is comparison by reference, which is implemented using the \lstinline|instanceof| operator. This method determines that an object is an instance of a class if there is a reference to that class, leveraging JavaScript's prototype system.

Another approach is comparison by structure, where two objects are considered equal if they share the same structure, meaning they have the same properties and methods. Kipper supports both methods of comparison. Reference-based comparison is implemented via the \lstinline|instanceof| operator and is exclusively used for class comparisons. Structural comparison, referred to as "matching", is applied to primitives and interfaces. 

Structural comparisons are implemented using the \lstinline|matches| operator as given in listing~\ref{lst:implementation:matchesoperator}.

\begin{lstlisting}[language=Typescript,caption=The Kipper matches operator,label=lst:implementation:matchesoperator]
interface Y {
	v: bool;
	t(gr: str): num;
}

interface X {
	y: Y;
	z: num;
}

var x: X = {
	y: {
		v: true,
		t: (gr: str): num -> {
			return 0;
		}
	},
	z: 5
};

var res: bool = x matches X; // -> true
\end{lstlisting}

As can be seen in listing~\ref{lst:implementation:matchesoperator}, the matches operator can compare interfaces by properties and methods. It takes two arguments, an object and a type which it should match. Properties are compared recursively and methods are compared by name, arguments and return type.

Comparison works by iterating over the methods and properties. When iterating over the properties, it checks for the property's name being present in the type it should check against. The order of properties does not matter. When the name is found, it checks for type equality. This checking is done using the aforementioned runtime types and nominal type comparison. In case a non-primitive is detected as the properties type, the matches function will be recursively executed on non-primitives.

This property match algorithm is implemented as given in listing~\ref{lst:implementation:matchesproperty}.

\begin{lstlisting}[language=Typescript,caption=Matches operator property comparison,label=lst:implementation:matchesproperty]
for (const field of pattern.fields) {
  const fieldName = field.name;
  const fieldType = field.type;

  if (!(fieldName in value)) {
    return false;
  }

  const fieldValue = value[fieldName];
  const isSameType = __kipper.typeOf(fieldValue) === field.type;

  if (primTypes.includes(field.type.name) && !isSameType) {
    return false;
  }

  if (!primTypes.includes(fieldType.name)) {
    if (!__kipper.matches(fieldValue, fieldType)) {
      return false;
    }
  }
}
\end{lstlisting}

After checking the properties, the matches expression iterates over the methods. It first searches for the method name in the target type. If found, it compares the return type. Then each argument is compared by name. As the methods signatures need to be exactly the same, the amount of parameters is compared as well.

\begin{lstlisting}[language=Typescript,caption=Matches operator method comparison,label=lst:implementation:matchesmethod]
for (const field of pattern.methods) {
  const fieldName = field.name;
  const fieldReturnType = field.returnType;
  const parameters = field.parameters;

  if (!(fieldName in value)) {
    return false;
  }

  const fieldValue = value[fieldName];
  const isSameType = fieldReturnType === fieldValue.__kipType.genericArgs.R;

  if (!isSameType) {
    return false;
  }

  const methodParameters = fieldValue.__kipType.genericArgs.T;

  if (parameters.length !== methodParameters.length) {
    return false;
  }

  let count = 0;
  for (let param of parameters) {
    if (param.type.name !== methodParameters[count].name) {
      return false;
    }
    count++;
  }
}
\end{lstlisting}

When none of these conditions are false, the input object matches the input type and they can be seen as compatible.

\subsection{Typeof Operator}
\label{subsec:typeof}

In the Kipper programming language, the \lstinline|typeof| operator is used to get the type of an object at runtime. This operator can be used to check if a variable or expression is of a particular type, such as a string, number, boolean, etc. Most commonly, it is used to check for null and undefined objects, to avoid type errors when an object is of unknown type. The returned type object can be compared by reference to check for type equality. As can bee seen in listing~\ref{lst:implementation:typeofoperator}, the parantheses are optional. We decided to allow both syntax styles, due to our goal of being similar to TypeScript and JavaScript, which both implement it the same way.

\begin{lstlisting}[language=Typescript,caption=Typeof operator used to determine the type of an input expression,label=lst:implementation:typeofoperator]
typeof 49; // "__kipper.builtIn.num"
typeof("Hello, World!"); // "__kipper.builtIn.str"
\end{lstlisting}

The  \lstinline|typeof| operator in Kipper mirrors the functionality of TypeScript and JavaScript, but with enhancements tailored to Kipper's type system. Unlike JavaScript, where the  \lstinline|typeof null| returns \lstinline|object| due to historical reasons, Kipper correctly identifies  \lstinline|null| as  \lstinline|__kipper.builtIn.null|.

At runtime, the provided object is checked for it's type using the target languages type features. A part of this process can be seen in listing~\ref{lst:implementation:typeofimplementation}. The primitive types return their respective \lstinline|KipperRuntimeType|. Objects are a special case, as they can either be null, an array, a class or an object, for example one that implements an interface.

\begin{lstlisting}[language=Typescript,caption=Logical implementation of the typeof operator in TypeScript, label=lst:implementation:typeofimplementation]
typeOf: (value) => {
    const prim = typeof value;
    switch (prim) {
        case 'undefined':
            return __kipper.builtIn.undefined;
        case 'string':
        return __kipper.builtIn.str;
        ...
        case 'object': {
            if (value === null) return __kipper.builtIn.null;
            if (Array.isArray(value)) {
                return '__kipType' in value ? value.__kipType : __kipper.builtIn.Array;
            }
            const prot = Object.getPrototypeOf(value);
            if (prot && prot.constructor !== Object) {
                return prot.constructor;
            }
            return __kipper.builtIn.obj;
        }
    }
}
\end{lstlisting}

Although linguistically quite similar, the \lstinline|typeof| operator in the type declaration of a variable works fundamentally different~\ref{lst:implementation:typeoftypespecifier}. It is called \lstinline|TypeOfTypeSpecifier| and it evaluates the type of a variable at compile time.

\begin{lstlisting}[language=Typescript,caption=Specifying the type based on a reference variable, label=lst:implementation:typeoftypespecifier]
var t: num = 3;
var count: typeof(t) = 4;
\end{lstlisting}

%%% Local Variables:
%%% mode: LaTeX
%%% TeX-master: "../thesis"
%%% End:
