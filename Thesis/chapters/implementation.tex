\section{Compiler}
\label{sec:compiler}
\setauthor{Luna Klatzer}

The Kipper Compiler is the core component of the Kipper project, serving as the central piece connecting the Kipper language to its target environment. It functions similarly to other \gls{transpilation}-based compilers, such as the TypeScript compiler, by producing high-level output code from high-level input codeâ€”specifically, code written in the Kipper language. The syntax of the language is predefined and implemented using the lexer and parser generated by the \Gls{antlr4} parser generator. 

An interesting aspect of the Kipper compiler is its largely modular design, which allows various components to be replaced or extended as needed. This modularity primarily serves to enable the compiler's structure to adapt to future changes in the language and its target output environment. Given the rapid evolution of web technologies and the frequent addition of new functionality, it is essential to ensure that the compiler remains current. Additionally, this design allows users to create plugins or extensions for the compiler to support custom functionality that may not be natively available.

Furthermore, as discussed in greater detail in section~\ref{sec:output-generation}, the compiler is capable of targeting more than one output format. Specifically, it supports both standard JavaScript, adhering to the ES6/ES2015 specification, and standard TypeScript as defined by Microsoft. Similar to other compiler systems, users can configure the compiler according to their preferences and specify the desired target language.

\subsection{Stages of compilation}
\setauthor{Luna Klatzer}

The Kipper compiler processes the program through multiple phases, each building upon the previous one to progressively enrich the semantic and logical representation of the program. The phases and their corresponding responsible components are as follows:

\begin{enumerate}
	\item Lexical Analysis~(See~\ref{sec:lexing-parsing})
	\begin{itemize}
		\item Performed by: Antlr4 Kipper Lexer
		\item This phase tokenizes the source code into a stream of lexemes, identifying the basic units of syntax.
	\end{itemize}
	\item Syntax Analysis - Parsing~(See~\ref{sec:lexing-parsing})
	\begin{itemize}
		\item Performed by: Antlr4 Kipper Parser
		\item In this phase, the lexed tokens are organized into a parse tree based on the grammar rules of the Kipper language.
	\end{itemize}
	\item Parse Tree Translation to AST~(See~\ref{sec:translation-to-the-ast})
	\begin{itemize}
		\item Performed by: Kipper Core Compiler
		\item Converts the parse tree into an Abstract Syntax Tree (AST), a more abstract and language-independent representation of the code.
	\end{itemize}
	\item Semantic Analysis~(See~\ref{sec:semantic-analysis})
	\begin{itemize}
		\item Performed by: Kipper Core Compiler
		\item This phase is split into three separate steps:
		\begin{enumerate}
			\item Primary Semantic Analysis: Validates language semantics such as variable declarations and scope resolution.
			\item Preliminary Type Analysis: This phase performs initial type validations and checks. It includes tasks such as loading type definitions that might be referenced elsewhere in the program. These types need to be pre-loaded to ensure they are available and correctly resolved during subsequent stages of compilation.
			\item Primary Type Analysis: Conducts in-depth type validation and resolves type-related issues for the given statements and expressions.
		\end{enumerate}
	\end{itemize}
	\item Target-Specific Requirement Checking
	\begin{itemize}
		\item Performed by: Target Semantic Analyser
		\item Ensures compliance with the specific requirements of the target platform (JavaScript or TypeScript).
	\end{itemize}
	\item Optimization
	\begin{itemize}
		\item Performed by: Kipper Core Compiler
		\item Performs code optimizations, currently focused on treeshaking to eliminate unused code.
	\end{itemize}
	\item Target-Specific Translation~(See~\ref{sec:output-generation})
	\begin{itemize}
		\item Performed by: Target Translator
		\item Translates the optimized AST into the desired target language (JavaScript or TypeScript).
	\end{itemize}
\end{enumerate}

Each phase of the compiler is executed sequentially, with each step requiring the successful completion of the previous one. This ensures that each module of the compiler can safely rely on the correctness and completeness of the information provided by earlier steps. However, this approach limits the compiler's ability to recover from errors or detect all faults in a single execution. As a trade-off, this method simplifies the implementation and reduces overall complexity. Unlike TypeScript, for example, this means that the compiler cannot ignore certain errors and work around them. 

\section{Lexing \& Parsing}
\label{sec:lexing-parsing}
\setauthor{Luna Klatzer}

The first step in the compilation process is the lexing and parsing of the input program. This involves the tokenization of the program source code, where individual strings are classified into predefined categories, followed by syntactical analysis that organizes these tokens into statements, expressions, and declarations.

In the Kipper compiler, these two steps are carried out by the Kipper Lexer and Parser generated by \Gls{antlr4}, rather than being directly implemented within the compiler itself. These \Gls{antlr4}-generated components are constructed based on the predefined token and syntax rules specific to the Kipper language.

\subsection{Syntax definition}

The primary utility provided by \Gls{antlr4} lies in its ability to generate lexers and parsers automatically from an input file written in the Antlr4-specific ".g4" context-free grammar format. For Kipper, the lexer and parser each have distinct definitions, specifying the individual tokens and the rules for constructing the syntax tree that organizes and groups these tokens.

These definitions are created in a manner similar to other syntactical specification methods, such as \acrshort{bnf} (Backus-Naur Form), commonly used for context-free formal grammars. However, unlike \acrshort{bnf}, Antlr4 grammars have the added capability to include programmatic conditions and invocations, enabling more dynamic and adaptable grammar definitions.

\subsubsection{Lexer Grammar Definition}
\label{sec:lexer-grammar-definition}

In the case of the Kipper Lexer, it uses its own token definition file, which defines how characters are grouped into tokens. This file specifies the rules for identifying individual tokens while ignoring special characters that are not required for syntax analysis. Additionally, the lexer separates the matched tokens into various channels, allowing for more efficient handling and categorization during subsequent parsing steps. (See~\ref{sec:token-channels} for more detail)

For example, in Kipper, the lexer grammar includes constructs for identifying both comments and language-specific keywords. These definitions can be seen showcased below in listing~\ref{lst:kipper-lexer}.

\begin{lstlisting}[language=antlr4, caption={Sample snippet from Kipper Lexer grammar}, label={lst:kipper-lexer}]
	BlockComment : '/' .? '*/' -> channel(COMMENT) ;
	
	LineComment : '//' CommentContent -> channel(COMMENT) ;
	
	Pragma : '#pragma' CommentContent -> channel(PRAGMA) ;
	
	InstanceOf : 'instanceof';
	
	Const : 'const';
	
	Var : 'var';
\end{lstlisting}

In the grammar above, comments are directed to a separate channel using the~\lstinline|-> channel| annotation. This helps isolate them from the main parsing flow while still retaining them for potential processing or documentation purposes. Pragmas, which are typically compiler directives, are handled similarly but redirected to a~\lstinline|PRAGMA| channel.

\Gls{antlr4} grammars also include support for defining keywords, operators, and contextual language constructs. For instance,~\lstinline|Const| and~\lstinline|Var| are tokenized as reserved keywords, ensuring they are recognized unambiguously during lexical analysis. This explicit tokenization is critical for constructing a clear and predictable syntax tree, which serves as the foundation for the subsequent phases of interpretation and compilation.

\subsubsection{Syntactical Grammar Definition}
\label{sec:parser-grammar-definition}

Like the Kipper Lexer, the Kipper Parser is defined by its own syntax definition file. In this file, various rules are grouped into syntax rules that collectively form a hierarchical structure. The parser traverses this structure to determine which syntactical construct is represented by the individual tokens. These syntax rules are organized in a way that allows the parser to construct a parse tree by following specific paths, with each path representing a distinct syntactical structure in the Kipper language.

For example, a grammar rule defining a function declaration in Kipper could be expressed as seen in listing~\ref{lst:function-declaration}:

\begin{lstlisting}[language=antlr4, caption={Function Declaration Grammar}, label={lst:function-declaration}]
	functionDeclaration : 'def' declarator '(' parameterList? ')' '->' typeSpecifierExpression compoundStatement? ;
	
	parameterList : parameterDeclaration (',' parameterDeclaration)* ;
	
	parameterDeclaration : declarator ':' typeSpecifierExpression ;
\end{lstlisting}

In this grammar, the rule for~\lstinline|functionDeclaration| begins with the keyword~\lstinline|'def'|, followed by a~\lstinline|declarator| (which represents the function name or identifier), and then an optional~\lstinline|parameterList|. The rule includes a~\lstinline|typeSpecifierExpression|, indicating the return type of the function, and optionally a~\lstinline|compoundStatement|, which represents the function's body.

The~\lstinline|parameterList| rule handles the optional inclusion of one or more parameters, each defined by~\lstinline|parameterDeclaration|. Each~\lstinline|parameterDeclaration| consists of a~\lstinline|declarator| (the parameter's name) followed by a type specification. This structure clearly defines the expected syntax for a function declaration in Kipper, ensuring that the parser can accurately identify and process this construct.

Using these syntax rules, the Kipper parser can build a detailed parse tree, with nodes representing various language constructs, such as function definitions, parameters, and expressions. This hierarchical structure allows for efficient interpretation or compilation of Kipper code.

\subsection{Lexical analysis}

\subsubsection{Primary tokenisation}

The primary task of the lexical analysis is the tokenization of the individual characters into grouped tokens which represent syntactical elements, such as identifiers, keywords, constant values, etc. Tokens serve as the building blocks for higher-level constructs in the parsing process, providing a simplified and structured representation of the raw source code.

The step of tokenization is fairly straightforward as it simply follows the definitions provided in the grammar file (See~\ref{sec:lexer-grammar-definition}) and throws errors in case no associated token definition is found. Each token is assigned a specific type based on the grammar rules, ensuring that the source code adheres to the language's syntactical structure. If a character sequence does not match any defined rule, an error is raised, indicating the presence of invalid syntax.

The lexer operates as a state machine, scanning through the input character stream and categorizing sequences based on patterns defined in the grammar. These patterns may include regular expressions to match identifiers, numerical constants, or specific language keywords. Once tokens are identified they are put into the associated channels as explained in~\ref{sec:token-channels}.

\begin{figure}[h!]
	\centering
	\def\stackalignment{r}
	\includegraphics[scale=1]{./pics/Lexer-Algorithm.drawio}
	\caption{The lexing process which categories the various tokens}
	\label{fig:implementation:Lexer-Algorithm}
\end{figure}

\subsubsection{Token Channels}
\label{sec:token-channels}

Next to the definition of the various tokens the grammar file also specifies what channel each token should be put into. These channels act as a stream of tokens where each stream represents different semantic parts of the program.

The channels which are implemented in the case of Kipper are:

\begin{itemize}
	\item \textbf{Default channel}
	\item \textbf{Comments channel}
	\item \textbf{Pragma channel}
	\item \textbf{Ignored channel}
\end{itemize}

The \textbf{default channel} serves as the primary stream, storing nearly all tokens in the program. It is the main channel used during the parsing step to construct a parse tree of the program.

The remaining channels are special-purpose streams that are excluded during parsing and cater to specific functionalities:

\begin{itemize}
	\item The \textbf{comments channel} is dedicated to storing all comments, which are logically irrelevant to the program and do not require parsing or further processing.
	\item The \textbf{pragma channel} contains compiler pragmasâ€”special instructions to the compiler that are processed independently from the standard syntax rules.
	\item The \textbf{ignored channel} is reserved for special characters that are significant only for token differentiation but have no logical relevance to the program, such as spaces. While spaces are critical for separating tokens, like with~\lstinline|var x| and~\lstinline|varx| they do not contribute to the program's logic and are therefore excluded from parsing.
\end{itemize}
	
\subsubsection{Nested Sub-Lexing}
\label{sec:nested-sub-lexing}

Besides standard sequential processing of the input, the Kipper Lexer also employs a technique called sub-lexing. Sub-lexing involves branching off the main lexing process and invoking a sub-lexer that operates under its own set of rules and guidelines. This specialized lexer handles specific subsets of tokens that require unique processing rules.

Sub-lexing is crucial for Kipper due to features such as templating, where code fragments are embedded within strings. In such cases, the lexer must correctly differentiate between string elements and code atoms (the inserted snippets inside the string). By using a simple push-and-pop mechanism, the lexers can function similarly to a stack, layering processing contexts on top of one another. Each context processes its corresponding string subset, enabling correct parsing of both regular syntax and embedded code within strings. This modular approach ensures cleaner handling of complex tokenization scenarios and increases the lexer's flexibility.

\begin{figure}[h!]
	\centering
	\def\stackalignment{r}
	\includegraphics[scale=0.8]{./pics/Sub-Lexer.drawio}
	\caption{The process of invocating a sub-lexer with the sample input~\lstinline|f"Result: \{sample + 4\}"| (An example of a template string, or also format string, in the Kipper language), where all content between~\lstinline|\{| and~\lstinline|\}| is passed onto the sub-lexer.}
	\label{fig:implementation:Sub-Lexer}
\end{figure}

\subsection{Syntactic analysis}

\subsubsection{Primary syntactic analysis of the token stream}

With the lexer having already identified all tokens in a given program and ensured that only valid elements are present, the parser proceeds to analyze the program's structure and logic. This step is inherently more complex and often demands a significant amount of processing time. The complexity arises partly from the design of \Gls{antlr4} and partly from the computational effort required to transform a token stream into a viable syntax tree.

The parser's primary task is to verify that the sequence of tokens conforms to the grammatical rules specified by the Kipper grammar. This involves checking whether constructs such as statements, expressions, and control structures are correctly formed.

\subsubsection{Programmatic conditions \& context-sensitive rules}

\subsection{AST (Abstract Syntax Tree)}
\label{sec:translation-to-the-ast}

\subsubsection{Parse Tree Walking}

\subsubsection{Tree cleanup}

\subsubsection{Utility provided by the AST}

\section{Semantic Analysis}
\label{sec:semantic-analysis}
\setauthor{Luna Klatzer}

\section{Type Analysis}
\label{sec:type-analysis}
\setauthor{Luna Klatzer}

\section{Error recovery}
\setauthor{Luna Klatzer}

The functionality of error recovery is integral to most modern compilers, as it allows the compiler to report multiple errors in a single compilation pass. Basic compilers often operate on an immediate fail-safe principle: upon encountering an error, the compilation process halts immediately to prevent the compiler from making incorrect assumptions about the program. While this approach ensures the integrity of the compilation process, it introduces a significant drawback. For large and complex programs, developers may need to repeatedly correct errors and recompile to uncover additional issues, leading to unnecessary delays and inefficiencies during the development process.

Given these issues, Kipper has implements its own error recovery algorithm into the semantic analysis which is able to recover from errors in given contexts and continue operation in following expressions or statements, which aren't associated with the original error.

\subsection{Error recovery algorithm}

\subsection{Special case: Syntax errors}

\section{Type System}
\setauthor{Fabian Baitura}

The type system is the primary element of a modern object-oriented programming languages and in the case of Kipper it is the primary area of attention.

\subsection{Intended Purpose \& Concept}

The purpose of a type system pre-defines the conditions and expectations of how the type system should act in various cases of ambiguity, type matching and derivative type definitions.

\subsection{Existing Foundation \& Environment}

As Kipper is a language that transpiles to either JavaScript or TypeScript (a superset of JavaScript), it is essential to consider the existing foundation and environment when designing and developing its type system. While Kipper is not as constrained as TypeScript, ensuring seamless interoperability and ease of translation with the underlying environment remains critical. This is particularly important since Kipper is designed to run both on the web and locally using Node.js or other JavaScript runtimes.

To achieve this, it is crucial to understand the foundation upon which JavaScript itself was built. JavaScript, created in 1995 by Brendan Eich, was initially envisioned as a lightweight scripting language to add interactivity to web pages. Its dynamic and loosely-typed nature provided flexibility and adaptability but also introduced susceptibility to errors in larger applications. Over time, JavaScript has been refined to improve its reliability and scalability for handling complex applications. However, its core type system has largely remained the same and hasn't been altered as a safe guard to ensure that backwards compatibility is preserved.

\subsubsection{Weak Dynamic Type System}

JavaScript is a dynamically typed language, meaning variables can hold values of different types during runtime. Unlike statically typed languages such as TypeScript or Java, JavaScript does not require explicit declaration of a variable's data type.

\begin{lstlisting}[caption=Holding Values of different Types during Runtime]
let foo = 10; // foo is a number
foo = "Hello"; // foo is now a string
foo = [1, 2, 3, 4, 5]; // foo is now an array
\end{lstlisting}

In contrast, TypeScript enforces static typing, ensuring that a variable's type remains consistent throughout its lifecycle. This prevents unintentional type changes and improves code reliability.

\begin{lstlisting}[caption=Statically Typed Language TypeScript]
let foo: number = 1; // x is explicitly declared as a number
foo = "Hello"; // Error: Type 'string' is not assignable to type 'number"
\end{lstlisting}

JavaScript is also a weakly typed language, meaning that it allows operations between different data types without the need for explicit type conversion. This flexibility can sometimes lead to unexpected results, as JavaScript automatically coerces values to the appropriate type when performing operations.

\begin{lstlisting}[caption=Automatic type conversion in JavaScript]
let quantity = 7; // quantity is an integer
let value = "20"; // value is a string

let total = quantity + value; // JavaScript automatically converts quantity to string
console.log(total); // Output: "720"
\end{lstlisting}

In the example above, the number 7 is stored in the variable quantity, and the string "20" is stored in the variable value. Typically, when attempting to add a number and a string, one might expect an error due to the mismatch in data types. However, JavaScript performs implicit type coercion, automatically converting the number into a string before performing the operation.

In this case, JavaScript converts the number 7 to the string "7" and concatenates it with the string "20", resulting in the string "720". This type conversion occurs implicitly, without the need for explicit instructions to JavaScript.

However, implicit type coercion can sometimes lead to unintended results if not handled carefully. It is important to understand how JavaScript performs these conversions to prevent unexpected behavior in your code. Awareness of these implicit conversions helps ensure that operations between different data types do not produce erroneous or undesirable outcomes.

\subsubsection{Primitives \& Core Types}

In JavaScript, a primitive (or primitive value, primitive data type) is a data type that is not an object and does not have methods or properties. There are seven primitive data types:

\begin{itemize}
	\item ~\lstinline|string|
	\item ~\lstinline|number|
	\item ~\lstinline|bigint|
	\item ~\lstinline|boolean|
	\item ~\lstinline|undefined|
	\item ~\lstinline|symbol|
	\item ~\lstinline|null|
\end{itemize}

Unique among the primitive types in JavaScript are undefined and null.

In JavaScript, null is considered a primitive value due to its seemingly simple nature. However, when using the typeof operator, it unexpectedly returns~\lstinline|object|. This is a known quirk in JavaScript and is considered a historical bug in the language that has been maintained for compatibility reasons.

\begin{lstlisting}[caption=typeof null return "object" in JavaScript]
	console.log(typeof null); // "object"
\end{lstlisting}

undefined is a primitive value automatically assigned to variables that have been declared but not yet assigned a value, or to formal function parameters for which no actual arguments are provided.

\begin{lstlisting}[caption=typeof null return "object" in JavaScript ]
	let item; // declare a variable without assigning a value

	console.log(`The value of item is ${item}`); // logs "The value of item is undefined"
\end{lstlisting}

All primitives in JavaScript are immutable, meaning they cannot be altered directly. It is important to distinguish between a primitive value and a variable that holds a primitive value. While a variable can be reassigned to a new value, the primitive value itself cannot be modified in the same way that objects, arrays, and functions can be altered. JavaScript does not provide utilities to mutate primitive values.

Primitives, such as numbers and strings, do not inherently have methods. However, they appear to behave as though they do, due to JavaScript's automatic wrapping, or "auto-boxing," of primitive values into their corresponding wrapper objects. For example, when a method like toString() is called on a primitive number, JavaScript internally creates a temporary Number object. The method is then executed on this object, not directly on the primitive value.

Consider the primitive value value = 10;. When a method such as value.toString() is invoked, JavaScript automatically wraps the primitive 10 in a Number object and calls Number.prototype.toString() on it. This behavior occurs invisibly to the programmer and serves as a helpful mental model for understanding various behaviors in JavaScript. For example, when attempting to "mutate" a primitive, such as assigning a property to a string (str.foo = 1), the original string is not modified. Instead, the value is assigned to a temporary wrapper object.

\subsubsection{Custom Dynamic Structures}

\subsubsection{Prototype Inheritance System}

\subsection{Translating the foundation to Kipper}

\subsection{Drawing comparisons to TypeScript}

\subsection{Kipper Primitives}

\subsection{Kipper Generics}

\subsection{Kipper Interfaces \& Duck-Typing}

\subsection{Kipper Classes \& Prototyping}

\section{Output Generation}
\label{sec:output-generation}
\setauthor{Lorenz Holzbauer}

\subsection{Introduction}

The Kipper compiler utilizes a modular architecture, allowing for the definition of custom targets and providing flexibility to accommodate various use cases. This modularity extends beyond basic configuration, enabling developers to specify target-specific features or behaviors. The modular design is achieved by dividing the compiler into two primary components: a frontend and a backend.

For comparison, the \acrshort{gcc} compiler achieves modularity by dividing its architecture into three components, as illustrated in Figure~\ref{fig:implementation:gcccompiler}. The frontend is responsible for verifying syntax and semantics, scanning the input, and performing type checking. It subsequently generates an intermediate representation (IR) of the code.

The middle-end then optimizes this intermediate representation, which is designed to be independent of the CPU target. Examples of middle-end optimizations include dead code elimination and detection of unreachable code. Finally, the backend takes the optimized intermediate code and generates target-dependent code. In the case of GCC, this involves generating assembly code.

\begin{figure}[h!]
	\centering
	\def\stackalignment{r}
	\stackunder{\includegraphics[scale=0.36]{./pics/Compiler_design}}{\scriptsize Source: \url{https://commons.wikimedia.org/wiki/File:Compiler_design.svg}}
	\caption{The design of the GCC compiler}
	\label{fig:implementation:gcccompiler}
\end{figure}

As of now, Kipper generates either TypeScript or JavaScript code as its output. The target language is specified in the Kipper CLI utility using the flag --target={js|ts}. If this flag is not provided, the default target is JavaScript.

Currently, Kipper does not include a middle-end component. This decision was made to avoid the additional complexity associated with implementing a full middle-end, as the resulting performance improvements in generated code would be only marginal. Instead, the frontend directly passes the \acrshort{ast} (Abstract Syntax Tree) to the backend. While Kipper does perform some post-analysis optimizations, these are presently limited to treeshaking.

\subsection{Abstraction during previous processing}

Kipper uses an \acrshort{ast} as the primary representation of the code from the input program. The \acrshort{ast} serves as a hierarchical structure that represents the program's source code. The root node of the \acrshort{ast} corresponds to the entire program, while each child node represents a specific construct such as a statement, scope, or other language feature.

Each node in the \acrshort{ast} contains the semantic data and type-related information of the corresponding statement, as well as a string representation of the original parser node. Additionally, every node includes a kind identification propertyâ€”a unique, hard-coded number in the compilerâ€”to uniquely identify the type of the node. This property aids in determining the type of construct, such as a declaration, an expression, or a statement.

Every node also maintains a list of child nodes, representing nested or dependent components of the construct. Once the \acrshort{ast} is fully constructed, it is wrapped and passed to the code generator for further processing.

\subsection{Algorithm}

Kipper processes the previously generated \acrshort{ast} and its nodes in a bottom-to-top manner. This approach ensures that the translation of child nodes is completed before their parent nodes are processed. Consequently, parent nodes can extract necessary information from their children. This is especially important for complex structures, as they depend on the code generated by their children, embedding it into their own output. The generated code is passed to the parent as an array of tokensâ€”simply output code in text form. This process is entirely safe since the \acrshort{ast} has already been validated for syntactical and semantic correctness during earlier phases. Each node is independently responsible for generating its own output code, a design choice that ensures modularity and flexibility.

The output generation in Kipper begins by setting up the target environment and generating any necessary requirements, as detailed in section~\ref{sec:requirements}. Once the setup is complete, the compiler iterates over the child nodes by calling the "translateCtxAndChildren" function. This function recursively traverses the tree, generating code for each child node. Each node returns a string containing its output code, which is then processed by its parent node. The process continues until the root node aggregates all the strings and sends the final merged output to a function that writes the code to a file.

The implementation of this translation algorithm is provided in example~\ref{lst:implementation:translationalgorithm}.

\begin{lstlisting}[language=TypeScript,caption=The translation algorithm,label=lst:implementation:translationalgorithm]
public async translate(): Promise<Array<TranslatedCodeLine>> {
	// SetUp and WrapUp functions
	const targetSetUp: TargetSetUpCodeGenerator = this.codeGenerator.setUp;
	const targetWrapUp: TargetWrapUpCodeGenerator = this.codeGenerator.wrapUp;

	// Add set up code, and then append all children
	const { global, local } = await this.programCtx.generateRequirements();
	let genCode: Array<TranslatedCodeLine> = [...(await targetSetUp(this.programCtx, global)), ...local];
	for (let child of this.children) {
		genCode.push(...(await child.translateCtxAndChildren()));
	}

	// Add wrap up code
	genCode.push(...(await targetWrapUp(this.programCtx)));

	// Finished code for this Kipper file
	return genCode;
}
\end{lstlisting}

The code generation function of a node takes the node as an argument and gets it's semantic and type-semantic data. This is then used to translate the children, which are properties of the semantic data, into source code. The result is then concatenated into a single string array and returned. This process can be seen in example ~\ref{lst:implementation:instanceofgeneration}. In this example, the instanceOf expression gets built.

Clearly visible is the same ''translateCtxAndChildren'' function as in example~\ref{lst:implementation:translationalgorithm}. This is the treewalker function. It visits every node in the AST and starts the code generation by the bottom-most node. It generates the code for a nodes children and then the node itself. This allows a node to control the order of compilation of their children.

\begin{lstlisting}[language=TypeScript,caption=The code generation function of a instanceOf expression,label=lst:implementation:instanceofgeneration]
instanceOfExpression = async (node: InstanceOfExpression): Promise<TranslatedExpression> => {
	const semanticData = node.getSemanticData();
	const typeData = node.getTypeSemanticData();
	const operand = await semanticData.operand.translateCtxAndChildren();
	const classType = TargetJS.getRuntimeType(typeData.classType);

    return [...operand, " ", "instanceof", " ", classType];
  };
\end{lstlisting}

The code generator functions of Kipper are implemented in a class called ''JavaScriptTargetCodeGenerator''. Due to the similarity between TypeScript and JavaScript, the TypeScript code generator extends the JavaScript code generator and overrides the functions that differ. This eliminates duplicate code fragments.

\subsection{Algorithms used for Output Generation}
There is a plethora of algoritms available to generate code in the target language. They can be classified by their input data structure. Some algoritms need a tree-like \acrshort{ir}, others need a linear \acrshort{ir} structure.

\subsubsection{Linear Algorithms}
Linear algorithms are most often used when compiling a high-level language to machine code or bytecode. They treat the input as a flat, ordered sequence and sequentially process it. Intermediate representations often are either three-address-code or static-single-assignment code. Linear algorithms process the code one instruction at a time in a sequence. When an instruction is complete, it gets pushed to a list, which in the end is concatenated and written to the output file. When using linear algorithms, there are two major ways of representing the \acrshort{ir}.

Three-address code consists of three operands and is typically an assignment and a binary operator ~\cite{wiki:threeaddress}. An instruction can have up to three operands, although it may have less. In the example ~\ref{lst:implementation:threeaddresscode}, the problem gets split up into multiple instructions. This allows the compiler to easily transform the instructions into assembly language or bytecode, which are similar in their structure. Furthermore, the compiler can identify unused code by checking if a variable gets used later on in the code.
\begin{lstlisting}[language=TypeScript,caption=Three-address code,label=lst:implementation:threeaddresscode]
// Problem
x = (-b + sqrt(b^2 - 4*a*c)) / (2*a)

// Solution
t1 := b * b
t2 := 4 * a
t3 := t2 * c
t4 := t1 - t3
t5 := sqrt(t4)
t6 := 0 - b
t7 := t5 + t6
t8 := 2 * a
t9 := t7 / t8
x := t9
\end{lstlisting}

The Static single-assignment form is a different intermediate representation, where each variable gets assigned exactly once \cite{wiki:singlestatic}. It is used in the most widely in compilers like \acrshort{gcc}. The main benefit is that it simplifies the code and improves the results of compiler optimizations. In example ~\ref{lst:implementation:staticsingleassignmentform}, the problem shows a variable ''y'' that gets assigned twice. The first assignment is therefore unnecessary. With the static single-assignment form, the compiler can determine that the assignment of ''y1'' was unnecessary, because it is never used in the code that comes after. Examples of optimizations that were enhanced by this form include dead-code elimination, constant propagation and register allocation.
\begin{lstlisting}[language=TypeScript,caption=Static single-assignment form,label=lst:implementation:staticsingleassignmentform]
// Problem
y := 1
y := 2
x := y

// Solution
y1 := 1
y2 := 2
x1 := y2
\end{lstlisting}

\subsubsection{Tree-based Algorithms}
Tree-based algorithms are often used in transpilers, as the code needs to stay human readable most of the time, and the general structure of the code should be preserved. This means, that the strucure of scopes and statements should roughly stay the same. Therefore the components are stored in a node in a tree-like structure. Kipper uses a bottom-up code generation algorithm. This means, that a treewalker recurses through the tree and generates the output starting from the most deeply nested node. We chose to use this method, because it was easier to visualize and implement and we wanted the source to stay human readable and extendable without aggressive optimizations and changes on the source. This is not possible with linear algorithms, as they transform the code either into the three-address code form or the static single-assignment form. While this happend, important information about the context and structure of the code gets lost.

Tree-based code generation can be used to generate bytecode. This bytecode gets optimized and further compiled by a linear algorithm. Tree-based algorithms have the disadvantage of allowing only local optimizations in the respective nodes. In addition, they can be computationally expensive, in case the input gets too complex, as the treewalker is a recursive function. Therefore, in professional compilers the tree-based design is not often used. \acrshort{gcc} uses a tree-based design in two of it's language independent \acrshort{ir}s, GIMPLE and GENERIC ~\cite{gcc:gimpletuples}.

\subsection{Requirements}
\label{sec:requirements}
Kipper is designed to have a runtime that is as small as possible while still having all the needed functionality bundled in it. This means, that the compiler should only include functions and objects into the runtime, that are needed by the user. This alligns with our goal of keeping the compiler modular and minimal. Due to the removal of unused components in a process called ''treeshaking'', the output code is kept small and efficient.

Kipper includes a range of built-in functions and features to support its runtime environment. These built-ins are organized and managed using a scoped approach. The global scope contains core runtime features that are always required, such as basic type handling and error reporting. Beyond the global scope, additional features are selectively included based on the specific requirements of the program being compiled.

\subsubsection{Conditional features}
Conditional features are runtime components that are included only when explicitly required by the program being compiled. These features can range from commonly used operations like match and slice to more specialized or program-specific utilities. Unlike essential runtime components housed in the global scope, conditional features are added selectively based on an analysis of the program's structure and functionality. The decision to include conditional features occurs during the requirements generation phase. As the compiler traverses the Abstract Syntax Tree (AST), it examines each node to determine whether a specific built-in function or runtime operation is invoked. If a feature like slice is used in the source code, it is flagged as necessary and included in the consolidated requirements list. This ensures that only the relevant components are integrated into the final runtime environment.

An example of a conditional feature would be the ''slice'' function ~\ref{lst:implementation:slicefunction}. This function takes an array as input and extracts a section starting from the index the first argument provides and ending at the index the second argument provides.

\begin{lstlisting}[language=TypeScript,caption=The Slice Operation,label=lst:implementation:slicefunction]
var valid: str = "321";
print(valid[1:2]); // 2
\end{lstlisting}

When the compiler encounters the slice operator, it registers the function as needed and therefore includes it into the runtime. This process can be seen in example~\ref{lst:implementation:sliceinternal}. The compiler calls slice function in the ''BuiltInGenerator''. The ''BuiltInGenerator'' is a collection of functions that generate the code for the built in functions in the target languages. This means that each target language has to have a full set of ''BuiltInGenerators''. The generator in the example generates the required JavaScript code for the function by utilizing the built in ''slice'' function of JavaScript.

\begin{lstlisting}[language=TypeScript,caption=Slice in the JavaScript BuiltInGenerator,label=lst:implementation:sliceinternal]
async slice(funcSpec: InternalFunction): Promise<Array<TranslatedCodeLine>> {
	const signature = getJSFunctionSignature(funcSpec);
	const objLikeIdentifier = signature.params[0];
	const startIdentifier = signature.params[1];
	const endIdentifier = signature.params[2];

	return genJSFunction(
		signature,
		`{ return ${objLikeIdentifier} ? ${objLikeIdentifier}.slice(${startIdentifier}, ${endIdentifier}) : ${objLikeIdentifier}; }`,
	);
  }
\end{lstlisting}

The generated "slice" function can be found in example ~\ref{lst:implementation:slicegenerated}.

\begin{lstlisting}[language=TypeScript,caption=Slice in the target language,label=lst:implementation:slicegenerated]
slice: function slice<T>(objLike: T, start: number | undefined, end: number | undefined): T {
	return objLike ? objLike.slice(start, end) : objLike;
  },
\end{lstlisting}

\subsubsection{The global scope}
The global scope in programming represents the top-level execution context where variables, functions, and objects are accessible throughout the entire runtime environment unless explicitly restricted. In JavaScript, the global scope is particularly significant as it varies across runtime environments like browsers, Node.js, and Web Workers. This variability necessitates robust mechanisms for identifying and managing the global context to ensure compatibility across environments.

For example, JavaScript defines several global objects, such as window in browsers, global in Node.js, and self in Web Workers. Modern JavaScript unifies these under globalThis, a standardized global object that provides a consistent way to access the global scope regardless of the environment. However, not all environments support globalThis, which is why fallback mechanisms are often used.

The Kipper global scope contains all the runtime features that are required. It is important, that the global scope exists only once, therefore the programm needs to check at runtime, if the scope already exists. This can be seen in example ~\ref{lst:implementation:globalscopelogic}. It first checks if ''\_\_globalScope'' is already defined and uses it if available. If not, it attempts to use globalThis, the modern standard JavaScript global object. If ''globalThis'' is not defined, it checks for window in browser environments, global in Node.js, or self in Web Workers. If none of these are defined, it falls back to an empty object. This ensures that the ''\_\_globalScope'' variable is always initialized, regardless of the environment, allowing consistent and safe access to the global scope.

\begin{lstlisting}[language=TypeScript,caption=Global Scope Logic,label=lst:implementation:globalscopelogic]
var __globalScope = typeof __globalScope !== "undefined" ? __globalScope :
    typeof globalThis !== "undefined" ? globalThis :
    typeof window !== "undefined" ? window :
    typeof global !== "undefined" ? global :
    typeof self !== "undefined" ? self : {};
\end{lstlisting}

\subsubsection{Internal functions}
Internal functions in Kipper serve as an essential part of the runtime, yet they are designed to remain hidden from the user-facing API. These functions provide support for various runtime operations and compiler processes but are not directly accessible or callable in the user's program. This ensures that the runtime environment remains clean and minimal while still delivering the required functionality.

An example of an internal function would be the ''assignTypeMeta'' function, which adds metadata to a runtime type. This is useful for runtime type comparison and described in detail in chapter ~\ref{subsec:builtintypes}. This function never gets exposed to the user but is called internally when an interface gets declared.

A key characteristic of internal functions is their dynamic inclusion in the runtime environment. During the requirements generation phase, the compiler identifies whether a program's functionality depends on any internal mechanisms. If so, the corresponding internal functions are included in the runtime.

\subsubsection{Requirements Generation}
The requirements generation process in Kipper begins during the compilation phase. As the compiler traverses the Abstract Syntax Tree (AST), it analyzes the nodes to determine the features needed by the program. This analysis produces a set of requirements, which are then used to configure the runtime environment. This works by using feature registration. If an AST node needs a certain feature, the reference is added to the program context by using the ''this.programCtx.addInternalReference'' function. When a feature is added more once, all further additions get ignored, as the function is already available. After all the features are registeres, the target generates the source code in the required language and inserts it into the output code.

\subsection{Differences between the Target Languages}
The implementation of a compiler or transpiler targeting multiple programming languages often requires handling the specific quirks and requirements of each target. As Kipper is a webdevelopment language, we target both TypeScript and JavaScript. Both languages share a common foundation but diverge significantly in their syntax rules, semantics, and type systems.

One of the primary challenges in supporting both JavaScript and TypeScript as target languages is their differing treatment of identifiers, reserved keywords, and type declarations. While JavaScript is dynamically typed and relatively permissive in terms of variable naming and usage, TypeScript enforces a stricter set of rules due to its static type-checking capabilities.

\subsubsection{Reserved Keywords}
Both JavaScript and TypeScript have a set of reserved keywords that cannot be used as identifiers. However, TypeScript introduces additional constraints by reserving type-related keywords, which are not present in JavaScript. For instance, class is a reserved keyword in both languages and cannot be used as a variable name. In contrast, TypeScript also reserves names like let, number, and other type names, making them invalid as variable or function names.

Kipper handles these reserved keywords by checking for them at compile time. The compiler compares against a hardcoded list of keywords and in case it finds one, it throws an ''ReservedIdentifierOverwriteError''. This list of keywords contains both the reserved words of JavaScript and TypeScript, as this minimizes redundancy and complexity. In addition to that, it also forces the developer to use sensible variable names, as JavaScript is quite leaneant with it's reserved keywords. example~\ref{lst:implementation:reservedkeywords} illustrates this.

\begin{lstlisting}[language=TypeScript,caption=Reserved Keywords in TS and JS,label=lst:implementation:reservedkeywords]
// Invalid in TypeScript
let let = 5;  // Error: Cannot use 'let' as an identifier
let number = 10;  // Error: Cannot use 'number' as an identifier

// Valid in JavaScript
var let = 5;  // No error
var number = 10;  // No error
\end{lstlisting}

\subsubsection{Type Annotations}
TypeScript introduces type annotations as part of its static type system. This means that while generating the TypeScript output, Kipper has to append type information in variable assignments, functions and lambdas. This works by overriding the JavaScript implementation of the code generator function and converting the AST-internal type of the node to a TypeScript type. Example ~\ref{lst:implementation:differencecodegeneratorfunctions} shows the difference between the JavaScript code generator function and the TypeScript code generator function for variable assignments. In the codeblock that generates TypeScript, there is an additional bit of code after the storage and the identifier, that inserts the type of the object. The function declaration and lambdas work similarly.

\begin{lstlisting}[language=TypeScript,caption=Difference in code generator functions,label=lst:implementation:differencecodegeneratorfunctions]
// JavaScript
return [
	[
		storage, " ",
		semanticData.identifier, ...(assign.length > 0 ? [" ", "=", " ", ...assign] : []), ";"
	]
];
// Result: let x = 5;

// TypeScript
return [
	[
		storage, " ", semanticData.identifier, ":", " ", tsType, // This inserts the type
		...(assign.length > 0 ? [" ", "=", " ", ...assign] : []),
		";",
	],
];
// Result: let x: number = 5;
\end{lstlisting}

\subsection{Stylistic Choices}
The syntax of Kipper is specifically designed to ease the transition of existing TypeScript and JavaScript developers to Kipper. Therefore it was important, to keep the output as similar to these languages as possible. We achieved this by adhering to the following principles.

\subsubsection{Human readable output}
The primary goal of Kipper's output generation is to produce code that mirrors human-written TS or JS as closely as possible. To achieve this, Kipper avoids unnecessary abstractions or layers that could obscure the intent of the code. For instance, variable names and function identifiers are preserved during transpilation without introducing machine-generated names or hashing schemes. This contrasts with languages like CoffeeScript, where the output, while functional, often requires familiarity with the transpiler's conventions to interpret effectively.

\subsubsection{No Code Compression}
Kipper explicitly avoids code compression techniques such as minification or inlining that can hinder readability. While compression is useful in production environments to reduce payload size, it is opposed to the goals of Kipper, as it negativly impacts code readability.

\subsubsection{Standardized Style Format}
Kipper enforces a standardized style format for its output to ensure consistency and predictability. It uses two spaces for block-level indentation to maintain clarity and avoid confusion with tab-based formatting. Braces are explicitly used for block delimiters, and semicolons terminate statements, adhering to common TypeScript/JavaScript conventions.

\subsubsection{Scope Visibility}
A critical aspect of Kipper's design is the clear representation of scopes in the generated output. Kipper employs explicit declarations such as let, const, and function to demarcate variable and function scopes. Indentation and brace placement further enhance the visual hierarchy, making it easy to identify nested scopes and understand their boundaries. This approach contrasts with languages like Python, where indentation alone determines scope, or languages like Lua, where scope visibility may rely on implicit conventions.

\subsubsection{Editable Code}
One of Kipper's unique selling points is that its transpiled output is not just readable but also editable. Developers can treat the generated TS/JS code as if it were written manually, enabling seamless integration with existing projects. By making the output editable, Kipper empowers developers to tailor the transpiled code to their specific needs without relying solely on the original Kipper source.

\subsubsection{Comparison with other languages}
CoffeeScript aimed to simplify JavaScript syntax but often produced output that was hard to debug due to its reliance on non-standard conventions. Kipper avoids these pitfalls by aligning its syntax and output with established TypeScript/JavaScript practices. While TypeScript generates clean and maintainable JavaScript, it requires a compilation step that may introduce additional complexity. Kipper simplifies this process by directly transpiling to both TypeScript and JavaScript, offering flexibility without sacrificing readability. Babel's output is highly optimized for compatibility but can be dense and difficult to modify. Kipper prioritizes maintainability over optimization, ensuring that the output remains approachable for developers.

\section{Integrated Runtime}
\label{sec:integrated-runtime}
\setauthor{Lorenz Holzbauer}

\subsection{Runtime Type Concept}

The primary goal of the Kipper runtime type system is to allow untyped values to be compared with defined types, such as primitives, arrays, functions, classes, and interfaces, removing any ambiguities that could cause errors. During code generation, all user-defined interfaces are converted into runtime types that store the information needed to perform type checks. These are then utilised alongside the built-in runtime types, such as ~\lstinline|num|,~\lstinline|str| or~\lstinline|obj|, to enable the compiler to add necessary checks and runtime references for any cast, match or typeof operation.

With the exception of interfaces, classes, and generics, types are primarily distinguished by their names. In these cases, type equality checks are performed using nominal comparisons, where the name acts as a unique identifier within the given scope e.g. type~\lstinline|num| is only assignable to~\lstinline|num|. For more complex structures, additional informationâ€”such as members or generic parametersâ€”is also considered.

In the case of interfaces, the names and types of fields and methods are used as discriminators. These fields and methods represent the minimum blueprint that an object must implement to be considered compatible with the interface and thus "assignable". In this regard, Kipper adopts the same duck-typing approach found in TypeScript.

For generics, which include~\lstinline|Array<T>| and~\lstinline|Func<T..., R>|, the identifier is used alongside the provided generic parameters to determine assignability. This ensures that when one generic is assigned to another, all parameters must match. For instance,~\lstinline|Array<num>| cannot be assigned to~\lstinline|Array<str>"|and vice versa, even if their overall structure is identical.

For user-defined classes, the compiler relies on the prototype to serve as the discriminator. In practice, this behaviour is similar to that of primitives, as different classes cannot be assigned to each other.

To ensure future compatibility with inheritance, Kipper also includes a "baseType" property, which allows types to be linked in an inheritance chain. However, this feature is currently unused.

\subsection{Runtime Type Implementations in other Languages}
\label{chap:runtime-other-languages}

\subsubsection{Nominal Type Systems}

Nominal type systems are used in most modern object-orientated programming languages like Java and C\#. In these systems, types are identified by their unique names and can only be assigned to themselves. Additionally, two types are considered compatible, if one type is a subtype of the other one, as can bee seen in listing~\ref{lst:implementation:javanominaltyping}. Here a ~\lstinline|Programmer| is an~\lstinline|Employee|, but not the other way around. This means~\lstinline|Programmer| instances have all the properties and methods an~\lstinline|Employee| has while also having additional ones specific to~\lstinline|Programmer| The relationships are as such inherited, so~\lstinline|SeniorDeveloper| is still an~\lstinline|Employee| and a~\lstinline|Programmer| at the same time. Even though the Senior Developer adds no new functionality to the~\lstinline|Programmer|, it is not treated the same. Nominal typing improves code readability and maintainability, due to the explicit inheritance declaration. On the other hand, this increases code redundancy for similar or even identical but not related structures.

\begin{lstlisting}[language=Java,caption=Example of nominal typing in Java,label=lst:implementation:javanominaltyping]
class Employee {
	public float salary;
}

class Programmer extends Employee {
	public float bonus;
}

class SeniorDeveloper extends Programmer { }
\end{lstlisting}

\subsubsection{Structural Type Systems}

Structural type systems compare types by their structure. This means, if two differently named types have the same properties and methods, then they are the same type. An example of this would be OCaml, with its object subsystem being typed this way. Classes in OCaml only serve as functions for creating objects. In example~\ref{lst:implementation:ocamlstructuraltyping} there is a function that requires a function~\lstinline|speak| returning the type~\lstinline|string|. Both the~\lstinline|dog| object as well as the~\lstinline|cat| object fulfill this condition, therefore both are treated equal. Most importantly, these compatibility checks happen at compile time, as OCaml is a static language. Structural typing allows for a lot of flexibility as it promotes code reuse. Furthermore it avoids explicit inheritance hierarchies.

\begin{lstlisting}[language=caml,caption=Example of structural typing in Ocaml,label=lst:implementation:ocamlstructuraltyping]
	let make_speak (obj : < speak : string >) =
	obj#speak

	let dog = object
	method speak = "Woof!"
	end

	let cat = object
	method speak = "Meow!"
	end

	let () =
	print_endline (make_speak dog);
	print_endline (make_speak cat);
\end{lstlisting}

\subsubsection{Duck Typed Systems - Duck Typing}

Duck Typing is the usage of a structural type system in dynamic languages. It is the practical application of the "Duck Test", therefore if it quacks like a duck, and walks like a duck, then it must be a duck. In programming languages this means that if an object has all methods and properties required by a type, then it is of that type. The most prominent language utilizing Duck Typing is TypeScript. As can be seen in listing~\ref{lst:implementation:javascriptducktyping}, the duck and the person have the same methods and properties, henceforth they are of the same type. The dog object on the other hand does not implement the "quack" function, which equates to not being a duck. Duck typing simplifies the code by removing type constraints, while still encouraging polymorphism without complex inheritance.

\begin{lstlisting}[language=Typescript,caption=Example of duck typing in TypeScript,label=lst:implementation:javascriptducktyping]
interface Duck {
	quack(): void;
}

const duck: Duck = {
	quack: function () {
		console.log("Quack!");
	}
};

const person: Duck = {
	quack: function () {
		console.log("I'm a person but I can quack!");
	}
};

const dog: Duck = {
	bark: function () {
		console.log("Woof!");
	}
}; // <- causes an error in the static type checker
\end{lstlisting}

Given that duck typing allows dynamic data to be easily checked and assigned to any interface, Kipper adopts a similar system to that of TypeScript but introduces notable differences in how interfaces behave and how dynamic data is handled. For instance, casting an~\lstinline|any| object to an interface in Kipper will result in a runtime error if the object does not possess all the required members. In contrast, TypeScript permits such an operation without performing any type checks at runtime.

\subsection{Runtime Base Type}
\label{subsec:basetype}

In practice, all user-defined and built-in types inherit from a basic~\lstinline|KipperType| class in the runtime environment. This class is a simple blueprint of what a type could do and what forms a type may take on. A simple version of such a class can be seen in listing~\ref{lst:implementation:runtimetypestructure}.

\begin{lstlisting}[language=TypeScript,caption=The structure of a runtime type,label=lst:implementation:runtimetypestructure]
	class KipperType {
		constructor(name, fields, methods, baseType = undefined, customComparer = undefined) {
			this.name = name;
			this.fields = fields;
			this.methods = methods;
			this.baseType = baseType;
			this.customComparer = customComparer;
		}

		accepts(obj) {
			if (this === obj) return true;
			return obj instanceof KipperType && this.customComparer ? this.customComparer(this, obj) : false;
		}
	}
\end{lstlisting}

As already mentioned types primarily rely on identifier checks to differentiate themselves from other types. Given though that there are slight differences in how types operate, they generally define themselves with what they are compatible using a comparator function. This comparator is already predefined for all built-ins in the runtime library and any user structures build on top of the existing rules established in the library.

Type~\lstinline|any| is an exception and is the only type that accepts any value you provide. However, assigning "any" to anything other than~\lstinline|any| is forbidden and it is necessary to cast it to a different type in order to use the stored value. By ~\lstinline|any| is as useless as possible, in order to force the developer into typechecking it.

Furthermore, classes are also exempt from this comparator behaviour, as classes behave like a value during runtime and provide a prototype which can simply be used to check if an object is an instance of that class.

\subsection{Runtime Built-in Types}
\label{subsec:builtintypes}

Built-in runtime types serve as the foundation of the type system and make up the parts of more complex constructs like interfaces. Built-in runtime types are compared at runtime by comparing their references, as they are uniquely defined at the start of the output code and available in the global scope. The implementations of such structures can be seen in the listing~\ref{lst:implementation:builtinruntimetypes} down below.

\begin{lstlisting}[language=TypeScript,caption=Examples for the built-in runtime types,label=lst:implementation:builtinruntimetypes]
	const __type_any =
	new KipperType("any", undefined, undefined);

	const __type_undefined =
	new KipperType("undefined", undefined, undefined, undefined, (a, b) => a.name === b.name);

	const __type_str =
	new KipperType("str", undefined, undefined, undefined, (a, b) => a.name === b.name);
\end{lstlisting}

In addition to the core primitive typesâ€”such as~\lstinline|bool|,~\lstinline|str|,~\lstinline|num|, and othersâ€”there are built-in implementations for generic types, including~\lstinline|Array<T>| and ~\lstinline|Func<T..., R>|. These additionally define their generic parameters which generally default to a standard~\lstinline|any| type as can be seen in listing~\ref{lst:implementation:genericbuiltintypes}.

\begin{lstlisting}[language=Typescript,caption=Generic built-in types,label=lst:implementation:genericbuiltintypes]
const __type_Array = new KipperGenericType("Array", undefined, undefined, {T: __type_any});
const __type_Func = new KipperGenericType("Func", undefined, undefined, {T: [], R: __type_any});
\end{lstlisting}

As can be seen in listing~\ref{lst:implementation:genericbuiltintypes}, generic types are implemented using a special~\lstinline|KipperGenericType| class. This class, shown in listing~\ref{lst:implementation:generickippertype}, extends the~\lstinline|KipperType| and includes an additional field for generic arguments. Most importantly, it includes the method~\lstinline|changeGenericTypeArguments|. which allows for modifying a type's generic arguments at runtime. It is used in lambda and array definitions, where the built-in generic runtime type is used and then modified to represent the specified generic parameters. When for example an array is initialized, it first gets assigned the default~\lstinline|Array<any>| runtime type, which is then modified by the ~\lstinline|changeGenericTypeArguments| method to create the required type, such ~\lstinline|Array<num>|. Arrays for example use the specified type for their elements, whilst functions require a return type as well as an array of argument types. The~\lstinline|Func<T..., R>| type on the other hand is used by lambda definitions, which are user-defined functions with a specific return type and arguments without a name.

\begin{lstlisting}[language=Typescript,caption=Generic Kipper Type,label=lst:implementation:generickippertype]
class KipperGenericType extends KipperType {
	constructor(name, fields, methods, genericArgs, baseType = null) {
		super(name, fields, methods, baseType);
		this.genericArgs = genericArgs;
	}
	isCompatibleWith(obj) {
		return this.name === obj.name;
	}
	changeGenericTypeArguments(genericArgs) {
		return new KipperGenericType(
		this.name,
		this.fields,
		this.methods,
		genericArgs,
		this.baseType
		);
	}
}
\end{lstlisting}

\subsection{Runtime Errors}

Other built-ins include error classes, which are used in the error handling system to represent runtime errors caused by invalid user operations. The base~\lstinline|KipperError| type has a name property and extends the target language's error type as can be seen in listing~\ref{lst:implementation:kippererrortypes}. Additional error types inherit this base type and extend it with additional error information. For instance, the ~\lstinline|KipperNotImplementedError| is used whenever a feature that is not yet implemented is used by the developer.

\begin{lstlisting}[language=Typescript,caption=Kipper error types,label=lst:implementation:kippererrortypes]
class KipperError extends Error {
	constructor(msg) {
		super(msg);
		this.name = "KipError";
	}
}
class KipperNotImplementedError extends KipperError {
	constructor(msg) {
		super(msg);
		this.name = "KipNotImplementedError";
	}
}
\end{lstlisting}

\subsection{Runtime Generation for Interfaces}

Unlike TypeScript, in Kipper all interfaces possess a runtime counterpart, which stores all the required information to verify type compatibility during runtime. This process is managed by the Kipper code generator, which adds custom type instances to the compiled code that represent the structures of the user-defined interfaces with all its methods and properties including their respective types.

Now take for example the given interfaces seen in listing~\ref{lst:implementation:inputinterface}.

\begin{lstlisting}[language=Typescript,caption=Example interfaces in the Kipper language,label=lst:implementation:inputinterface]
	interface Car {
		brand: str;
		honk(volume: num): void;
		year: num;
	}

	interface Person {
		name: str;
		age: num;
		car: Car;
	}
\end{lstlisting}

At compile time, the generator function iterates over the interface's members and differentiates between properties and methods. The function keeps separate lists of already generated runtime representations for properties and methods.

If it detects a property, the type and semantic data of the given property is extracted. When the property's type is a built-in type, the respective runtime type already provided by the Kipper runtime library is used. If not, we can assume the property's type is a reference to another type structure, which will be simply referenced in our new type structure. This data is stored in an instance of~\lstinline|__kipper.Property|, which is finally added to the list of properties in the interface.

In case a method is detected, the generator function fetches the return type and the method's name. If the method has any arguments, the name and type of each argument also gets evaluated and then included in the definition of the~\lstinline|__kipper.Method|. After that, it gets added to the interface as well and is stored in its own separate method list.

If we translate the interfaces shown above in listing~\ref{lst:implementation:inputinterface} it would look similar to that in listing~\ref{lst:implementation:runtimeinterface}.

\begin{lstlisting}[language=Typescript,caption=The runtime representation of the previous interfaces,label=lst:implementation:runtimeinterface]
	const __intf_Car = new __kipper.Type(
		"Car",
		[
			new __kipper.Property("brand", __kipper.builtIn.str),
			new __kipper.Property("year", __kipper.builtIn.num),
		],
		[
			new __kipper.Method("honk", __kipper.builtIn.void,
				[
					new __kipper.Property("volume", __kipper.builtIn.num),
				]
			),
		]
	);

	const __intf_Person = new __kipper.Type(
		"Person",
		[
			new __kipper.Property("name", __kipper.builtIn.str),
			new __kipper.Property("age", __kipper.builtIn.num),
			new __kipper.Property("car", __intf_Car),
		],
		[]
	);
\end{lstlisting}

As shown in listing~\ref{lst:implementation:runtimeinterface}, the properties and methods of an interface are encapsulated within a~\lstinline|KipperType| instance, identified by the ~\lstinline|__intf_| prefix. The code for this runtime interface is included directly in the output file, where it can be accessed by any functionality that requires it. To reference the generated interface, the compiler maintains a symbol table that tracks all defined interfaces. The code generator then inserts runtime references to these interfaces wherever necessary.

Notable usages for runtime typechecking include the~\lstinline|matches| operator \ref{subsec:matches} and the~\lstinline|typeof| operator \ref{subsec:typeof}.

\subsection{Matches Operator for Interfaces}
\label{subsec:matches}

The primary feature of the Kipper programming language is its runtime type comparison. There are multiple approaches for comparing objects at runtime. One method is comparison by reference, which is implemented using the~\lstinline|instanceof| operator. This method determines that an object is an instance of a class if there is a reference to that class, leveraging JavaScript's prototype system.

Another approach is comparison by structure, where two objects are considered equal if they share the same structure, meaning they have the same properties and methods. Kipper supports both methods of comparison. Reference-based comparison is implemented via the~\lstinline|instanceof| operator and is exclusively used for class comparisons. Structural comparison, referred to as "matching", is applied to primitives and interfaces. Structural comparisons are implemented using the matches operator as can been seen in listing~\ref{lst:implementation:runtimeinterface}.

\begin{lstlisting}[language=Typescript,caption=The Kipper matches operator,label=lst:implementation:matchesoperator]
interface Y {
	v: bool;
	t(gr: str): num;
}

interface X {
	y: Y;
	z: num;
}

var x: X = {
	y: {
		v: true,
		t: (gr: str): num -> {
			return 0;
		}
	},
	z: 5
};

var res: bool = x matches X; // -> true
\end{lstlisting}

As can be seen in example~\ref{lst:implementation:matchesoperator}, the matches operator can compare interfaces by properties and methods. It takes two arguments, an object and a type which it should match. Properties are compared recursively and methods are compared by name, arguments and return type.

Comparison works by iterating over the methods and properties. When iterating over the properties, it checks for the property's name being present in the type it should check against. The order of properties does not matter. When the name is found, it checks for type equality. This checking is done using the aforementioned runtime types and nominal type comparison. In case a non-primitive is detected as the properties type, the matches function will be recursively executed on non-primitives.

This property match algorithm is implemented as can seen in listing~\ref{lst:implementation:matchesproperty}.

\begin{lstlisting}[language=Typescript,caption=Matches operator property comparison,label=lst:implementation:matchesproperty]
for (const field of pattern.fields) {
  const fieldName = field.name;
  const fieldType = field.type;

  if (!(fieldName in value)) {
    return false;
  }

  const fieldValue = value[fieldName];
  const isSameType = __kipper.typeOf(fieldValue) === field.type;

  if (primTypes.includes(field.type.name) && !isSameType) {
    return false;
  }

  if (!primTypes.includes(fieldType.name)) {
    if (!__kipper.matches(fieldValue, fieldType)) {
      return false;
    }
  }
}
\end{lstlisting}

After checking the properties, the matches expression iterates over the methods. It first searches for the method name in the target type. If found, it compares the return type. Then each argument is compared by name. As the methods signatures need to be exactly the same, the amount of parameters is compared as well.

\begin{lstlisting}[language=Typescript,caption=Matches operator method comparison,label=lst:implementation:matchesmethod]
for (const field of pattern.methods) {
  const fieldName = field.name;
  const fieldReturnType = field.returnType;
  const parameters = field.parameters;

  if (!(fieldName in value)) {
    return false;
  }

  const fieldValue = value[fieldName];
  const isSameType = fieldReturnType === fieldValue.__kipType.genericArgs.R;

  if (!isSameType) {
    return false;
  }

  const methodParameters = fieldValue.__kipType.genericArgs.T;

  if (parameters.length !== methodParameters.length) {
    return false;
  }

  let count = 0;
  for (let param of parameters) {
    if (param.type.name !== methodParameters[count].name) {
      return false;
    }
    count++;
  }
}
\end{lstlisting}

When none of these condition is false, the input object matches the input type and they can be seen as compatible.

\subsection{Typeof Operator}
\label{subsec:typeof}

In the Kipper programming language, the~\lstinline|typeof| operator is used to get the type of an object at runtime. This operator can be used to check if a variable or expression is of a particular type, such as a string, number, boolean, etc. Most commonly, it is used to check for null and undefined objects, to avoid type errors when an object is of unknown type. The returned type object can be compared by reference to check for type equality. As can bee seen in example~\ref{lst:implementation:typeofoperator}, the parantheses are optional. We decided to allow both syntax styles, due to our goal of being similar to TypeScript and JavaScript, which both implement it the same way.

\begin{lstlisting}[language=Typescript,caption=Typeof operator used to determine the type of an input expression,label=lst:implementation:typeofoperator]
typeof 49; // "__kipper.builtIn.num"
typeof("Hello, World!"); // "__kipper.builtIn.str"
\end{lstlisting}

The ~\lstinline|typeof| operator in Kipper mirrors the functionality of TypeScript and JavaScript, but with enhancements tailored to Kipper's type system. Unlike JavaScript, where the ~\lstinline|typeof null| returns~\lstinline|object| due to historical reasons, Kipper correctly identifies ~\lstinline|null| as ~\lstinline|__kipper.builtIn.null|.

At runtime, the provided object is checked for it's type using the target languages type features. A part of this process can be seen in example~\ref{lst:implementation:typeofimplementation}. The primitive types return their respective~\lstinline|KipperRuntimeType|. Objects are a special case, as they can either be null, an array, a class or an object, for example one that implements an interface.

\begin{lstlisting}[language=Typescript,caption=Logical implementation of the typeof operator in TypeScript, label=lst:implementation:typeofimplementation]
typeOf: (value) => {
    const prim = typeof value;
    switch (prim) {
        case 'undefined':
            return __kipper.builtIn.undefined;
        case 'string':
        return __kipper.builtIn.str;
        ...
        case 'object': {
            if (value === null) return __kipper.builtIn.null;
            if (Array.isArray(value)) {
                return '__kipType' in value ? value.__kipType : __kipper.builtIn.Array;
            }
            const prot = Object.getPrototypeOf(value);
            if (prot && prot.constructor !== Object) {
                return prot.constructor;
            }
            return __kipper.builtIn.obj;
        }
    }
}
\end{lstlisting}

Although linguistically quite similar, the~\lstinline|typeof| operator in the type declaration of a variable works fundamentally different~\ref{lst:implementation:typeoftypespecifier}. It is called~\lstinline|TypeOfTypeSpecifier| and it evaluates the type of a variable at compile time.

\begin{lstlisting}[language=Typescript,caption=Specifying the type based on a reference variable, label=lst:implementation:typeoftypespecifier]
var t: num = 3;
var count: typeof(t) = 4;
\end{lstlisting}


%%% Local Variables:
%%% mode: LaTeX
%%% TeX-master: "../thesis"
%%% End:
